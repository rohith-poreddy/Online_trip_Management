<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Managing persistent volume access in Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/17/managing-persistent-volume-access-kubernetes" /><author><name>Don Schenck</name></author><id>338525ef-ccb2-43d7-b529-4639c7dbd9b9</id><updated>2021-11-17T07:00:00Z</updated><published>2021-11-17T07:00:00Z</published><summary type="html">&lt;p&gt;Data storage gets complex in the world of &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;, as we discussed in &lt;a href="https://developers.redhat.com/articles/2021/08/11/how-maximize-data-storage-microservices-and-kubernetes-part-1-introduction"&gt;Part 1 of this series&lt;/a&gt;. That article explained the &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; concept of a &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"&gt;persistent volume (PV)&lt;/a&gt; and introduced &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation"&gt;Red Hat OpenShift Data Foundation&lt;/a&gt; as a simple way to get persistent storage for your applications running in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Beyond the question of provisioning storage, one must think about types of storage and access. How will you read and write data? Who needs the data? Where will it be used? Because these questions sound a bit vague, let's jump into some specific examples.&lt;/p&gt; &lt;p&gt;I ran the examples in this article on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, a free instance of OpenShift that you can use to begin your OpenShift and Kubernetes journey.&lt;/p&gt; &lt;h2&gt;Data on the ROX: ReadOnlyMany&lt;/h2&gt; &lt;p&gt;Read access is the basic right granted by any data source. We'll start with read access and examine writing in a later section.&lt;/p&gt; &lt;p&gt;There are a couple of variants of read access in Kubernetes. If your application reads data from a source but never updates, appends, or deletes anything in the source, the access is &lt;code&gt;ReadOnly&lt;/code&gt;. And, because &lt;code&gt;ReadOnly&lt;/code&gt; access never changes the source, multiple consumers can read from the source at the same time without risk of receiving corrupted results, a type of access called &lt;code&gt;ReadOnlyMany&lt;/code&gt; or ROX.&lt;/p&gt; &lt;p&gt;When a persistent volume claim (PVC) is created, the technology upon which it exists is specified by the storage class used in the YAML configuration file. When you ask for a PVC, you specify a size, a storage class, and an access method such as ROX. It is up to the underlying technology to meet the access method's requirements. And not every technology meets the access requirements of ROX. For example, &lt;a href="https://aws.amazon.com/ebs/"&gt;Amazon Elastic Block Store (EBS)&lt;/a&gt; does not.&lt;/p&gt; &lt;p&gt;If you create a PVC using a storage class that does not support your access method, you do not receive any error message at creation time. It is not until you assign the PVC to an application that any error message appears. For example, through the following configuration, I created a PVC called &lt;code&gt;claim2&lt;/code&gt; that tried to use EBS with a &lt;code&gt;ReadOnlyMany&lt;/code&gt; access level:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim2 namespace: rhn-engineering-dschenck-dev spec: accessModes: - ReadOnlyMany volumeMode: Filesystem resources: requests: storage: 1Gi&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's what happened at the command line. OpenShift was happy to oblige me in the creation of this PVC, even though it won't accept data at runtime:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;PS C:\Users\dschenck\Desktop&gt; oc create -f .\persistentvolumeclaim-claim2.yaml persistentvolumeclaim/claim2 created&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 1 shows the claim waiting for assignment.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/pending.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/pending.png?itok=bugMY_7q" width="740" height="282" alt="Openshift allows the PVC to be created." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. PVC waiting in Pending status.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I then created an app and assigned this PVC to it. That is when the error appeared, as seen in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/error.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/error.png?itok=VnW_kEXf" width="925" height="104" alt="When the application tries to get access to the PV, an error message is displayed." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Error message from PV says that the access mode is not supported. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If the PVC had been based on a storage class that supported ROX, this error would not have appeared.&lt;/p&gt; &lt;p&gt;To avoid crucial runtime problems such as the one I've laid out here, DevOps practices are valuable. Developers and operators work together to identify problems early in the development process.&lt;/p&gt; &lt;h2&gt;ReadWriteOnce (RWO) and ReadWriteMany (RWX)&lt;/h2&gt; &lt;p&gt;Reading and writing data is supported by the &lt;code&gt;ReadWriteOnce&lt;/code&gt; (RWO) and &lt;code&gt;ReadWriteMany&lt;/code&gt; (RWX) access methods. As you can surmise, these methods are dictated by the storage method that underlies the storage class. Regardless of which access method you choose, you must make sure the storage class supports it. Two storage classes that support RWX access are CephFS and the Network File System (NFS).&lt;/p&gt; &lt;p&gt;Creating file system storage and using CephFS is facilitated by theÂ &lt;a href="https://www.redhat.com/en/resources/openshift-data-foundation-overview"&gt;OpenShift Data Foundation (ODF) Operator&lt;/a&gt;, which not only provisions the storage but brings with it a lot of benefits. Snapshots and backups are simple operations with ODF and can be automated as well. If you're going to provision storage, you need to bring along some sort of method for backup and recovery.&lt;/p&gt; &lt;h2&gt;Object storage&lt;/h2&gt; &lt;p&gt;Traditionally, operating systems and storage media have offered file systems and block storage. One of these storage types underlies the storage of regular files (e.g., &lt;code&gt;readme.txt&lt;/code&gt;) as well as relational databases and NoSQL databases.&lt;/p&gt; &lt;p&gt;But what if you need to store objects such as videos and photos that benefit from large amounts of metadata? A video might have a title, an author, a subject, tags, length, screen format, etc. Storing this metadata with the object is preferable to, say, a database entry associated with the video.&lt;/p&gt; &lt;h3&gt;Object Bucket Claims&lt;/h3&gt; &lt;p&gt;OpenShift accommodates object storage by allowing you to create an Object Bucket Claim (OBC) object. An OBC is compatible with &lt;a href="https://aws.amazon.com/s3/"&gt;AWS S3&lt;/a&gt; storage, which means that any existing object-storage code you have will likely work with OBC, needing few if any changes.&lt;/p&gt; &lt;p&gt;Using object storage means you can use an S3 client library, such as &lt;a href="https://aws.amazon.com/sdk-for-python/"&gt;Boto3&lt;/a&gt; with &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;. Metadata can be stored with an object as a collection of key-value pairs. This is the storage model of choice for items such as photos, videos, and music.&lt;/p&gt; &lt;h3&gt;The Multicloud Object Gateway&lt;/h3&gt; &lt;p&gt;But we can go deeper. The Multicloud Object Gateway (MCG) gives you more options for object storage.&lt;/p&gt; &lt;p&gt;The most powerful aspect of MCG is that it allows you to store your data across multiple cloud providers using the AWS S3 protocol. MCG is able to interact with the following types of backing stores to physically store the encrypted and deduplicated data:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://aws.amazon.com/s3/"&gt;AWS S3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/cloud/object-storage"&gt;IBM Cloud Object Storage (IBM COS)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/services/storage/blobs/"&gt;Azure Blob Storage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://cloud.google.com/storage"&gt;Google Cloud Storage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A local PV-based backing store&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The MCG is a layer of abstraction that separates the developer from the implementation, making data portability a real possibility. As a developer, I'm good with that.&lt;/p&gt; &lt;p&gt;MCG gives you options. As mentioned by my colleague, Michelle DiPalma, in her excellent video &lt;a href="https://www.redhat.com/en/about/videos/understanding-multicloud-object-gateway"&gt;Understanding Multicloud Object Gateway&lt;/a&gt;, MCG enables three aspects of object storage:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Flexibility across multiple backends&lt;/li&gt; &lt;li&gt;Consistent experience everywhere&lt;/li&gt; &lt;li&gt;Simplified management of siloed data&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This is done by using three different bucket options:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;MCG object buckets (mirroring, spread across resources)&lt;/li&gt; &lt;li&gt;MCG dynamic buckets (OBC/OB inside your cluster)&lt;/li&gt; &lt;li&gt;MCG namespace buckets: data federated buckets&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;MCG object buckets ensure data safety. Your data is mirrored and spread across as many resources as you configure. You have three options for a storage architecture:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Multicloud: A mix of AWS, Azure, and GCP, for example&lt;/li&gt; &lt;li&gt;Multisite: Multiple data centers within the same cloud&lt;/li&gt; &lt;li&gt;Hybrid: Can facilitate large-scale on-premises data storage and cloud-based mirroring, for example&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Namespace buckets carry out data federation: Assembling many sources into one. Several data resources (e.g., Azure and AWS) are pulled together into one view. While the system architect decides where the data is written by an application, the location is hidden from the developers. They simply write to a named bucket; MCG takes care of writing the data and propagating it across the configured storage pool.&lt;/p&gt; &lt;h3&gt;Inside object storage&lt;/h3&gt; &lt;p&gt;OBC buckets are set up inside your cluster and are created using your application's YAML file. When you ask for an OBC, the OBC and the containing object bucket are dynamically created with permissions that you refer to in your application via environment variables. The environment variables, in a sense, bind the application and the OBC together.&lt;/p&gt; &lt;p&gt;Think of an OBC as the object storage sibling of a PVC. The OBC is persistent and is stored inside your cluster. OBC is great when you want developers to start using object storage without needing the overhead or time necessary for the final implementation (e.g., namespace buckets).&lt;/p&gt; &lt;h2&gt;Learn more about container storage&lt;/h2&gt; &lt;p&gt;This article is just an introduction to storage options for containers. There are many resources for you to dive into, learn more, and get things up and running. Here is a list to get you started:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The aforementioned video by Michelle DiPalma, &lt;a href="https://www.redhat.com/en/about/videos/understanding-multicloud-object-gateway"&gt;Understanding Multicloud Object Gateway&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.8"&gt;Product Documentation for Red Hat OpenShift Container Storage 4.8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://cloud.redhat.com/blog/introducing-multi-cloud-object-gateway-for-openshift"&gt;Introducing Multi-Cloud Object Gateway for OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation"&gt;Red Hat OpenShift Data Foundation&lt;/a&gt;Â (datasheet and access options)&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Writing applications for OpenShift means dealing with data. Hopefully, this article series will help you reach your goals.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/17/managing-persistent-volume-access-kubernetes" title="Managing persistent volume access in Kubernetes"&gt;Managing persistent volume access in Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2021-11-17T07:00:00Z</dc:date></entry><entry><title type="html">Auditing case management executions with Kafka and Red Hat Process Automation Manager</title><link rel="alternate" href="https://blog.kie.org/2021/11/auditing-case-management-executions-with-kafka-and-red-hat-process-automation-manager.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/11/auditing-case-management-executions-with-kafka-and-red-hat-process-automation-manager.html</id><updated>2021-11-16T20:40:53Z</updated><content type="html">Case management provides problem resolution for dynamic processes as opposed to the efficiency-oriented approach of BPM for routine, predictable tasks. It manages one-off situations when the process flow is determined based on the incoming request. Red Hat Process Automation Manager provides the ability to define Case Management applications using the BPMN 2.0 notation. In this article, we will explore how you can capture audit metrics for a running case instance. Using Case Listeners for fine-grained and async auditing Case Events Listener can be used to capture notifications for case-related events and operations that are invoked on a case instance. This can then be sent downstream to analytical tools. The Case Events listener can be implemented by overriding any of the methods as defined by the interface.Â  In our example, we will set up a listener to capture the following events: * Case start, * Case close, * Comments added and * Case data added. We will then send them over to a Kafka topic from which this data can be visualized or analyzed. private void pushToKafka(CaseDefinition caseDefinition) { try { Future&lt;RecordMetadata&gt; out = producer.send(new ProducerRecord&lt;String, String&gt;("case_events", caseDefinition.getCaseId(), new ObjectMapper().writeValueAsString(caseDefinition))); } catch (JsonProcessingException e) { e.printStackTrace(); } } @Override public void afterCaseStarted(CaseStartEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Started",null,null, new Date()); pushToKafka(caseDefinition); } @Override public void finalize() { System.out.println("case listener clean up"); producer.flush(); producer.close(); } @Override public void afterCaseDataAdded(CaseDataEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Data Added",event.getData(),null, new Date()); pushToKafka(caseDefinition); }; @Override public void afterCaseDataRemoved(CaseDataEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Data Removed",event.getData(),null, new Date()); pushToKafka(caseDefinition); }; @Override public void afterCaseClosed(CaseCloseEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Closed",null,null, new Date()); pushToKafka(caseDefinition); }; @Override public void afterCaseCommentAdded(CaseCommentEvent event) { CaseComment caseComment = new CaseComment(event.getComment().getComment(),event.getComment().getAuthor()); CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Comments Added",null,caseComment,new Date()); pushToKafka(caseDefinition); }; @Override public void afterCaseReopen(CaseReopenEvent event) { CaseDefinition caseDefinition = new CaseDefinition(event.getCaseId(),"Case Reopened",null,null, new Date()); pushToKafka(caseDefinition); } Notice how we extract the event data properties that we are interested in so that we can push it for analysis. We will then package the listener class as a maven project so that we can configure it on our case project. A complete example of the listener can be found in this . Configuring the listener on the case project: Now that we have created the listener, we will now configure it in our case project. First, we should add the listener jar to business central so that our case project can use it. You can use Business Central UI to upload the jar file. The following Aritfact upload option can be acessed through the menu CentralÂ  â Settings âArtifacts. Upload the jar file: Now, letâs add the dependency for the listener jar on our case project. You can do this by accessing, Business Central in Menu â Design â PROJECT_NAME â Settings âDependencies Next, you can configure the listener using the deployment descriptors. Access it in: Business Central in Menu â Design â PROJECT_NAME â Settings â Deployments. Finally, we can build and deploy the changes, and the listener should be able to capture case changes as they occur. Visualizing the collected data In order to visualize the data, let us set up a simple UI application. This reads from the kafka topic where we push our case metrics and shows it on a responsive UI. The application can be started using:Â  mvn quarkus:dev The UI application should be available at Testing the Case Audit Metrics: Let us create a case request. We can now see that audit metrics start populating on the UI application we created. Notice how the case start and case data added events have been captured. For every data element added to the case file, the event defines the payload associated with the data added. Similarly, other case changes like comments being added and cases being closed can be captured similarly. Summary This simple demo project for case listeners and the UI can be used with any case project. It shows how we can set up a listener for a case, and how we could push it down to Kafka for effective monitoring and audit traceability. References: The post appeared first on .</content><dc:creator>Sadhana Nandakumar</dc:creator></entry><entry><title type="html">Complex KIE server tests automation part 2: Deadline notifications after KIE server restart</title><link rel="alternate" href="https://blog.kie.org/2021/11/deadline-notifications-after-restart.html" /><author><name>Gonzalo MuÃ±oz FernÃ¡ndez</name></author><id>https://blog.kie.org/2021/11/deadline-notifications-after-restart.html</id><updated>2021-11-16T16:53:42Z</updated><content type="html">âDeadlines keep you on track for successâ (Timothy Dalton) MOTIVATION: SHOULDNâT WE AUTOMATE RESILIENCE TESTING AS WELL? This article follows the . In this ocasion, we will cover how to test deadline notifications after KIE server restart. With human task deadlines, you can set a time limit for that task, when not started or completed. Therefore, the KIE server will send a notification (by default, an automated email) to the target people defined for that task as managers, administrators, other team members. Here, we will focus on resilience testing. If the KIE server crashes, as there is a mechanism to persist the timer in a database, the KIE server will recover it after restarting. Consequently, the failover mechanism allows sending the deadline notification (triggered before crashing) at the right timing. The following figure depicts this scenario: This flow involves multiple system testing components: KIE Server, database, and SMTP server for mailing. We might be tempted to do a one-time setup instead of creating an automated test for our CI/CD pipelines. But, in the spirit of Martin Fowlerâs soundbite (reduces pain drastically), we will follow the automated approach. This way we will exercise the failover scenario regularly, taking advantage of containers.Â  First of all, we have to select our tools wisely, as the SMTP server. ONE SMTP SERVER TO TRACK THEM ALL The purpose of notification testing is to ensure that: * right inbox receives the expected emails (no more, no less). * the received emails have the right content for headers and body. So, we need a containerized SMTP server with the following requirements: * Easy to configure: minimum setup for required operations (e.g., authentication is useless to check testing emails) * Fast bootstrap: not too long to be up and running * Clear API to assist test automation: documented REST APIs to retrieve and clear emails. In our case, we have chosen ââ, an open-source SMTP server, as it fulfills these expectations and also has a docker image published in . We use the following REST APIs in our automation. Respectively, they retrieve all the messages from the inbox and delete all the messages after each test: Notice that they belong to different versions of the API (there is no DELETE operation at v2). Therefore, two different basePaths (/api/v2, /api/v1) will be set up for each endpoint in the REST-assured RequestSpecification. FITTING CONTAINERS TOGETHER Once we have defined our testing approach with self-unit independent pieces (containers) and the testing frameworks (Junit5 with and ), it is time to write our tests that will be the glue for fitting all the components together.Â  Our system-under-test -the main component- is the KIE Server. From version 7.61.0.Final, we can download it from the . From a Multistage Dockerfile, a temporary image on-the-fly will be created containing the business application (kjar) for exercising test scenarios. Therefore: * First stage, it will pull the slim maven image and install the kjars tailored for our tests. * Second stage, it will pull the KIE server image (in this case, ) with any additional configuration (jboss-cli scripts for configuring persistence and logging). The mailhog container is a testcontainers GenericContainer (that internally pulls the latest image from docker hub). It is exposing their SMTP and HTTP ports. Notice that it shares the same network as the rest of the containers: communication can occur among them without the need of exposing ports through the host. @Container public static GenericContainer&lt;?&gt; mailhog = new GenericContainer&lt;&gt;("mailhog/mailhog:latest") .withExposedPorts(PORT_SMTP, PORT_HTTP) .withNetwork(network) .withNetworkAliases("mailhog") .withLogConsumer(new Slf4jLogConsumer(logger)) .waitingFor(Wait.forHttp("/")); Finally, the PostgreSQL container is one of the out-of-the-box testcontainers database modules. We will use the initialization script under /docker-entrypoint-initdb.d containing postgresql-jbpm-schema.sql as explained . Maven build-tool pulls the schema from GitHub sources, from the same branch as the system-under-test. It is automatically downloaded using the download-maven-plugin at generate-sources phase, as it is shown in this snippet taken from the pom.xml: &lt;configuration&gt; &lt;url&gt;http://raw.githubusercontent.com/kiegroup/jbpm/${version.org.kie}/jbpm-db-scripts/src/main/resources/db/ddl-scripts/postgresql/postgresql-jbpm-schema.sql&lt;/url&gt; &lt;outputFileName&gt;postgresql-jbpm-schema.sql&lt;/outputFileName&gt; &lt;unpack&gt;false&lt;/unpack&gt; &lt;outputDirectory&gt;${project.build.directory}/postgresql&lt;/outputDirectory&gt; &lt;/configuration&gt; TESTING DEADLINE NOTIFICATIONS AFTER KIE SERVER RESTART This scenario (deadline notifications after KIE server restart) in former releases of KIE Server contained a bug tracked by :Â  duplicated emails were received in the target inboxes. The root cause was that the initialization of the human task service occurred before the deployment (upon server restart). Therefore, the timer service was not available at the moment of starting deadlines, provoking the problem of double notification. Letâs try to verify that, after fixing, it is working properly. First of all, we need a kjar containing a simple process with a human task with this deadline configuration: We can easily stop the KIE Server container after deploying the kjar and start it again to simulate a crash with a reboot.Â  And then, after deadlines timeout, we will check the expected outcome by means of REST-assured utils. We can connect to Mailhog to assure that the received emails match the expected ones: given() .spec(specV2) .when() .get("/messages") .then() .body("total", equalTo(3)) .assertThat().body("items[0].Content.Headers.To", hasItem("administrator@jbpm.org")) .assertThat().body("items[0].Content.Headers.From", hasItem("john@jbpm.org")) .assertThat().body("items[0].Content.Headers.Subject", hasItem("foo")) .assertThat().body("items[0].Content.Body", is("bar")); We also have another test for checking the case when a different kjar is deployed after the KIE server restart, then, no notification is sent because the kjar that triggered the deadline is not deployed again. Finally, you can find the code and configuration for this example . CONCLUSION: AUTOMATED TESTS FOR DEADLINE NOTIFICATIONSÂ AFTER KIE SERVER RESTART Automated Tests are also a good option to make resilience and integration testing less painful. We can combine different dockerized components (from the system-under-test -KIE server- to external databases -PostgreSQL- or SMTP servers -Mailhog-) into JUnit tests. As we control the containersâ lifecycle, it is easy to stop/start them to check failover mechanisms. The post appeared first on .</content><dc:creator>Gonzalo MuÃ±oz FernÃ¡ndez</dc:creator></entry><entry><title>Custom JFR event templates with Cryostat 2.0</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/16/custom-jfr-event-templates-cryostat-20" /><author><name>Andrew Azores, Elliott Baron</name></author><id>e48bf3e8-2a3c-4f9d-82e8-ad6ba4e344a5</id><updated>2021-11-16T07:00:00Z</updated><published>2021-11-16T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome back to our series of hands-on introductions to using &lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Cryostat 2.0&lt;/a&gt;. This article shows you how to preconfigure custom &lt;a href="https://github.com/cryostatio/cryostat#event-templates"&gt;event templates&lt;/a&gt; for Java application monitoring with JDK Flight Recorder (JFR). First, you'll use the new &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; and a Red Hat OpenShift &lt;code&gt;ConfigMap&lt;/code&gt; to create a custom event template, then you'll instruct the Cryostat Operator to use the &lt;code&gt;ConfigMap&lt;/code&gt; when deploying Cryostat. You can use the OpenShift console to interact with Cryostat or edit the YAML file for your Cryostat custom resource. We'll demonstrate both approaches.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Cryostat is JDK Flight Recorder for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The &lt;a href="https://access.redhat.com/documentation/en-us/openjdk/11/html/release_notes_for_cryostat_2.0"&gt;Red Hat build of Cryostat 2.0&lt;/a&gt; is now widely available in technology preview. Cryostat 2.0 introduces many new features and improvements, such as automated rules, a better API response JSON format, custom targets, concurrent target JMX connections, WebSocket push notifications, and more. The Red Hat build includes the &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; to simplify and automate Cryostat deployment on OpenShift.&lt;/p&gt; &lt;h2&gt;Create a custom template and ConfigMap&lt;/h2&gt; &lt;p&gt;With Cryostat, you can &lt;a href="https://cryostat.io/guides/#download-edit-and-upload-a-customized-event-template"&gt;download&lt;/a&gt; existing template files directly from a Java virtual machine (JVM). Once you've downloaded a template file to your local machine, you can use the &lt;a href="https://cryostat.io/guides/#edit-template-with-jmc"&gt;JDK Mission Control&lt;/a&gt; (JMC) Template Manager to easily edit the template to suit your needs.&lt;/p&gt; &lt;p&gt;After customizing the template file, it's fairly simple to create a &lt;code&gt;ConfigMap&lt;/code&gt; from it. This &lt;code&gt;ConfigMap&lt;/code&gt; stores the template file inside of the cluster where Cryostat will run. You can use &lt;code&gt;oc&lt;/code&gt; or &lt;code&gt;kubectl&lt;/code&gt; in the namespace where Cryostat is to be deployed. Here's the source if you use the former:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ oc create configmap my-template --from-file=/path/to/custom.jfc &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command creates a &lt;code&gt;ConfigMap&lt;/code&gt; named &lt;code&gt;my-template&lt;/code&gt; from a file on your local machine, located at &lt;code&gt;/path/to/custom.jfc&lt;/code&gt;. The file will be placed inside the &lt;code&gt;ConfigMap&lt;/code&gt; under the &lt;code&gt;custom.jfc&lt;/code&gt; filename.&lt;/p&gt; &lt;h2&gt;Add your ConfigMap to the Cryostat Operator&lt;/h2&gt; &lt;p&gt;Once you've created a &lt;code&gt;ConfigMap&lt;/code&gt;, you only need to instruct the Cryostat Operator to use it when deploying Cryostat. You can do this when you create a Cryostat custom resource, or by updating an existing one. If you're installing Cryostat using the OpenShift console, select &lt;strong&gt;Event Templates&lt;/strong&gt; under the Cryostat custom resource that you want to update. Choose &lt;strong&gt;Add Event Template&lt;/strong&gt;. The &lt;strong&gt;Config Map Name&lt;/strong&gt; drop-down list will display all &lt;code&gt;ConfigMap&lt;/code&gt;s in the local namespace. Select the &lt;code&gt;ConfigMap&lt;/code&gt; containing the event template you just created. For &lt;strong&gt;Filename&lt;/strong&gt;, enter the name of the &lt;code&gt;.jfc&lt;/code&gt; file within the &lt;code&gt;ConfigMap&lt;/code&gt;. In Figure 1, we're using &lt;code&gt;custom.jfc&lt;/code&gt;, which we created in the previous section.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/create-cryostat-template-config-map.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/create-cryostat-template-config-map.png?itok=McevdC0z" width="600" height="236" alt="Creating an OpenShift ConfigMap containing a JDK .jfc event template definition file." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Create a ConfigMap with a JDK .jfc event template definition file. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can also work directly with the YAML for the Cryostat custom resource. The process is similar to what you did in the console. Add an &lt;code&gt;eventTemplates&lt;/code&gt; property to the &lt;code&gt;spec&lt;/code&gt; section, if one isn't already present. Then, add an array entry with &lt;code&gt;configMapName&lt;/code&gt; referencing the name of the &lt;code&gt;ConfigMap&lt;/code&gt;, and a &lt;code&gt;filename&lt;/code&gt; referencing the filename within the &lt;code&gt;ConfigMap&lt;/code&gt;. The YAML below mirrors what we did in the OpenShift console example in Figure 1.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; apiVersion: operator.cryostat.io/v1beta1 kind: Cryostat metadata: Â Â name: cryostat-sample spec: Â Â eventTemplates:Â  Â Â - configMapName: custom-template Â Â Â Â filename: my-template.jfc &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Once you've saved your changes to the Cryostat custom resource, the Cryostat Operator will deploy Cryostat with this template preconfigured. Visiting the Cryostat web application will show that the template is present and available to use for creating new JDK flight recordings, as you can see in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cryostat-template-from-config-map.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/cryostat-template-from-config-map.png?itok=FxLG5uud" width="600" height="136" alt="The Event Template list displays the template created via ConfigMap." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The Event Template list displays the template created via ConfigMap. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you've learned how to preconfigure Cryostat with customized event templates using OpenShift &lt;code&gt;ConfigMap&lt;/code&gt;s. You can use customized templates to gather the specific data you need in your JDK flight recordings. Visit &lt;a href="http://cryostat.io/"&gt;Cryostat.io&lt;/a&gt; and see the other articles in this series for further details:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/25/introduction-to-containerjfr-jdk-flight-recorder-for-containers"&gt;Introduction to Cryostat: JDK Flight Recorder for containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Get started with Cryostat 2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/26/configuring-java-applications-use-cryostat"&gt;Configuring Java applications to use Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/02/java-monitoring-custom-targets-cryostat"&gt;Java monitoring for custom targets with Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="ADD_URL"&gt;Automating JDK Flight Recorder in containers&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/16/custom-jfr-event-templates-cryostat-20" title="Custom JFR event templates with Cryostat 2.0"&gt;Custom JFR event templates with Cryostat 2.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andrew Azores, Elliott Baron</dc:creator><dc:date>2021-11-16T07:00:00Z</dc:date></entry><entry><title>.NET 6 now available for RHEL and OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/15/net-60-now-available-rhel-and-openshift" /><author><name>Mauricio "Maltron" Leal</name></author><id>871d7b48-0893-4b7c-8e36-132292329b44</id><updated>2021-11-15T20:00:00Z</updated><published>2021-11-15T20:00:00Z</published><summary type="html">&lt;p&gt;.NET 6 is now generally available on &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux (RHEL)&lt;/a&gt; 7, RHEL 8, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. Here's a quick overview of what developers need to know about this new major release.&lt;/p&gt; &lt;h2&gt;New features in .NET 6&lt;/h2&gt; &lt;p&gt;In addition to x64 architecture (64-bit Intel/AMD), &lt;a href="https://developers.redhat.com/topics/dotnet/"&gt;.NET&lt;/a&gt; is now also available for ARM64 (64-bit ARM), and s390x (64-bit IBM Z) architectures.&lt;/p&gt; &lt;p&gt;.NET 6 includes new language versions &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-10)"&gt;C# 10&lt;/a&gt; and &lt;a href="https://devblogs.microsoft.com/dotnet/whats-new-in-fsharp-6/"&gt;F# 6&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;ASP.NET Core adds a new &lt;a href="https://devblogs.microsoft.com/aspnet/asp-net-core-updates-in-net-6-preview-4/#introducing-minimal-apis"&gt;minimal API&lt;/a&gt; that leverages new C# 10 features to write web applications with less code.&lt;/p&gt; &lt;p&gt;Like previous versions, .NET 6 brings many &lt;a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6/"&gt;performance improvements&lt;/a&gt; to the base libraries, GC and JIT.&lt;/p&gt; &lt;p&gt;.NET 6 introduces source generators for &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-net-6-preview-4/#microsoft-extensions-logging-compile-time-source-generator"&gt;logging&lt;/a&gt; and &lt;a href="https://devblogs.microsoft.com/dotnet/try-the-new-system-text-json-source-generator/"&gt;JSON&lt;/a&gt;. Thanks to these generators, JSON serialization and logging can be performed with less allocation and better performance.&lt;/p&gt; &lt;h2&gt;How to install .NET 6&lt;/h2&gt; &lt;p&gt;You can install .NET 6 on RHEL 7 (x64 only) with the usual command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-html"&gt;# yum install rh-dotnet60&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On RHEL 8 (for x64, arm64, and s390x), enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-html"&gt;# dnf install dotnet-sdk-6.0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The .NET 6 SDK and runtime container images are available from the Red Hat Container Registry. You can use the container images as standalone images and with OpenShift on all supported architectures:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman run --rm registry.redhat.io/ubi8/dotnet-60 dotnet --version 6.0.100&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Long-term support for .NET 6&lt;/h2&gt; &lt;p&gt;.NET 6 is a long-term support (LTS) release. It will be supported for three years, until November 2024.&lt;/p&gt; &lt;p&gt;Based on the .NET release schedule, the next version of .NET, .NET 7, is not an LTS release. It will be released in November 2022 and supported for 18 months until May 2024.&lt;/p&gt; &lt;p&gt;The next LTS release is .NET 8, which will be released in November 2023.&lt;/p&gt; &lt;p&gt;The existing .NET Core 3.1 and .NET 5 releases will be supported until December 2022 and May 2022, respectively.&lt;/p&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;Visit the &lt;a href="http://redhatloves.net/"&gt;.NET overview&lt;/a&gt; page to find out more about using .NET on Red Hat Enterprise Linux and OpenShift. You can also explore &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;more .NET resources on Red Hat Developer&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Learn &lt;a href="https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift#"&gt;three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm"&gt;Deploy .NET applications on Red Hat OpenShift using Helm&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/15/net-60-now-available-rhel-and-openshift" title=".NET 6 now available for RHEL and OpenShift"&gt;.NET 6 now available for RHEL and OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mauricio "Maltron" Leal</dc:creator><dc:date>2021-11-15T20:00:00Z</dc:date></entry><entry><title type="html">Monitoring Quarkus runtime with DevUI</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/monitoring-quarkus-runtime-with-devui/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=monitoring-quarkus-runtime-with-devui" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/monitoring-quarkus-runtime-with-devui/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=monitoring-quarkus-runtime-with-devui</id><updated>2021-11-15T17:26:22Z</updated><content type="html">Quarkus now includes (since version 2.3.0) a Development UI (DevUI) which allows us to monitor runtime information about your extensions. Quarkus DevUI is an experimental feature you can use to collect inner details of your Quarkus environment. You can use for different purposes such as: Application introspection, to see the list of Beans / Observers ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now generally available</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/15/red-hat-software-collections-38-and-red-hat-developer-toolset-11-now-generally" /><author><name>Brian Gollaher</name></author><id>62c08ed0-9672-4d63-92c8-4441f1f231cb</id><updated>2021-11-15T07:00:00Z</updated><published>2021-11-15T07:00:00Z</published><summary type="html">&lt;p&gt;The latest versions of&lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt; Red Hat Software Collections&lt;/a&gt; and&lt;a href="https://developers.redhat.com/products/developertoolset/overview"&gt; Red Hat Developer Toolset&lt;/a&gt; for &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 7 are now generally available. Software Collections 3.8 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to the &lt;a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux"&gt;worldâs leading enterprise Linux platform&lt;/a&gt;. These components are supported for up to five years, helping to enable a more consistent, efficient, and reliable developer experience.&lt;/p&gt; &lt;p&gt;Here's a quick overview of the updated development tools and collections you'll find in this release.&lt;/p&gt; &lt;h2 id="updates_to_red_hat_software_collections-h2"&gt;Updates to Red Hat Software Collections&lt;/h2&gt; &lt;p&gt;New and updated collections in the latest release of Red Hat Software Collections include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Nginx 1.20:&lt;/strong&gt;Â This is a new release of Nginx, a web and proxy server with a focus on high concurrency, performance, and low memory usage. This version supports client SSL certificate validation with Online Certificate Status Protocol (OCSP) and improves support for HTTP/2.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Redis 6:&lt;/strong&gt; This release of Redis, a persistent key-value database, supports SSL on all channels, as well as access control list and Redis Serialization Protocol version 3.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;JDK Mission Control 8.0.1 (update):&lt;/strong&gt;Â JDK Mission Control is an advanced set of tools for managing, monitoring, profiling, and troubleshooting &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; applications. This update to JMC 8 delivers a number of important bug and security fixes.&lt;/li&gt; &lt;/ul&gt;&lt;h2 id="updates_to_red_hat_developer_toolset-h2"&gt;Updates to Red Hat Developer Toolset&lt;/h2&gt; &lt;p&gt;Also new in Red Hat Software Collections 3.8 is Developer Toolset 11, which brings an updated, curated collection of &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;compilers&lt;/a&gt;, toolchains, debuggers, and other critical development tools. Forming the foundation of Developer Toolset 11 is GCC 11.2, a new update of the popular open source compiler collection. Additional updates in Developer Toolset 11 deliver new features to &lt;a href="https://developers.redhat.com/topics/c/"&gt;C/C++&lt;/a&gt; and Fortran debugging and performance tools.&lt;/p&gt; &lt;p&gt;In addition to the Developer Toolset, other compiler toolsets available in RHEL developer tools are updated with this release. Go Toolset is updated to version 1.16, LLVM Toolset is updated to version 12.0, and Rust Toolset is updated to version 1.54.&lt;/p&gt; &lt;p&gt;New collections in Red Hat Software Collections 3.8 are also available as&lt;a href="https://connect.redhat.com/explore/red-hat-container-certification"&gt; Red Hat Certified Containers&lt;/a&gt; through the &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy mission-critical applications using the supported components of Red Hat Software Collections for Red Hat Enterprise Linux and&lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt; Red Hat OpenShift&lt;/a&gt; environments.&lt;/p&gt; &lt;h2&gt;How to access Red Hat Software Collections 3.8&lt;/h2&gt; &lt;p&gt;Red Hat Software Collections 3.8 continues Red Hatâs commitment to customer choice in terms of the underlying compute architecture, with availability across x86_64, ppc64, ppc64le, and s390x hardware.&lt;/p&gt; &lt;p&gt;Red Hat customers with active&lt;a href="https://developers.redhat.com/blog/2019/08/21/why-you-should-be-developing-on-red-hat-enterprise-linux/"&gt; Red Hat Enterprise Linux&lt;/a&gt; subscriptions can access Red Hat Software Collections via the&lt;a href="https://access.redhat.com/solutions/472793"&gt; Red Hat Software Collections repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For more information, please read the full&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3-beta/"&gt; &lt;/a&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3/"&gt;release notes&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/15/red-hat-software-collections-38-and-red-hat-developer-toolset-11-now-generally" title="Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now generally available"&gt;Red Hat Software Collections 3.8 and Red Hat Developer Toolset 11 now generally available&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Brian Gollaher</dc:creator><dc:date>2021-11-15T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.4.2.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-4-2-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-4-2-final-released/</id><updated>2021-11-12T00:00:00Z</updated><published>2021-11-12T00:00:00Z</published><summary type="html">Today, we released Quarkus 2.4.2.Final, a maintenance release for our 2.4 release train containing bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.4. If you are not using 2.4 already, please refer to the 2.4 migration guide. Full changelog You can get the full changelog...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2021-11-12T00:00:00Z</dc:date></entry><entry><title type="html">Getting started with Hibernate reactive on Quarkus</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-hibernate-reactive/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=getting-started-with-hibernate-reactive" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-hibernate-reactive/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=getting-started-with-hibernate-reactive</id><updated>2021-11-11T19:30:19Z</updated><content type="html">This tutorial will introduce you to Hibernate Reactive which enables support for non-blocking database drivers and a reactive programming with Hibernate ORM. Uni and Multi streams Persistence operations are designed to use blocking IO for interaction with the database, and are therefore not appropriate for use in a reactive environment. Hibernate Reactive is the first ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Red Hat Summit Connect and Partner Experience Dublin - Talking Architecture Shop (slides)</title><link rel="alternate" href="http://www.schabell.org/2021/11/red-hat-summit-connect-and-partner-experience-slides.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/11/red-hat-summit-connect-and-partner-experience-slides.html</id><updated>2021-11-11T17:30:00Z</updated><content type="html">TheÂ Â and events for customers were held this week in Dublin. Each was a one day event, Â part of a new series of small-scale events, that brings the discussion of open source technology to your local cities. The days were intended to bring updates and insights into the latest technologies and also offered the opportunity to get hands on with a few Red Hat technologies. Â There were different streams of session topics hosting tech talks led by Red Hat experts and also business focused sessions delivered from local industry leaders including some fantastic partner and customer stories. I was invited to give a session at both events in Dublin this week and wanted to share the slides presented that included links to all of the available content we discussed.Â  As a refresh on the day, we shared our insights into our larger architectures exploring how open source can be used successfully at scale. Below you will find the slides: Provided here as a reminder, the title and abstract: You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise implementations that scale? This session takes attendees on a tour of multiple use cases covering enterprise challenges like integration, optimisation, cloud adoption, hybrid cloud management, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences.Â  The attendee departs this session with a working knowledge of how to map general open source technologies to their solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop!Â  Hope you enjoyed the session and if you were not able to attend, maybe we can meet and chat about this in person someday in the near future.</content><dc:creator>Eric D. Schabell</dc:creator></entry></feed>
