<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">DevConf.US 2021 - Designing your best architecture diagrams workshop</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/5c5h8SaqLp8/devconfus-2021-designing-your-best-architecture-diagrams-workshop.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/LMdo9vWm8Ds/devconfus-2021-designing-your-best-architecture-diagrams-workshop.html</id><updated>2021-08-04T14:00:00Z</updated><content type="html">ended their call for papers this last month and has announced acceptance for sessions to be hosted on September 2-3.  It's the 4th annual, free, Red Hat sponsored technology conference for community project and professional contributors to Free and Open Source technologies coming to a web browser near you! There is no admission or ticket charge for DevConf.US events. However, you are . Talks, presentations and workshops will all be in English. I had submitted a few talks and workshops and here are my acceptances that arrived this week. Designing your best architecture diagrams (workshop) Diagraming is one of the most important communication tools for sharing your project and architectural ideas to your colleagues and teams. In this workshop attendees are walking step-by-step through using an open source tool we host online for designing architecture diagrams like an expert. Attendees work through the following: * open and explore the tooling in your favourite web browser * explore the provided asset libraries for drag-and-drop designing * learn about the three types of diagrams that make up a good design * create your first simple logical diagram * create your first simple schematic diagram * create a detailed diagram * how to export diagrams and elements from a diagram * design tips and tricks Each of the individual labs in this workshop are stand alone, allowing the attendee to focus on anything of interest without having to work through the previous labs. If you're looking to become more proficient in sharing your ideas, architectures, and projects visually to wider audiences you can't underestimate the value of a good diagram. Join us to learn the tips and tricks that make a good diagram such a good communication vehicle and how our tooling eases your design tasks.  The workshop is fully implemented and tested with designers and architects around the world and fine tuned to the beginning designer. Workshop is  for your previewing pleasure. The schedule has yet to be posted, so keep an eye on the site for exact times if you'd like to attend. Hope to see you there!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/5c5h8SaqLp8" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/LMdo9vWm8Ds/devconfus-2021-designing-your-best-architecture-diagrams-workshop.html</feedburner:origLink></entry><entry><title>Managing stateful applications with Kubernetes Operators in Golang</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/o1kPvgHNpS4/managing-stateful-applications-kubernetes-operators-golang" /><author><name>Priyanka Jiandani</name></author><id>a090a17f-26ad-49fb-b2e0-ee1c6d9092c8</id><updated>2021-08-04T07:00:00Z</updated><published>2021-08-04T07:00:00Z</published><summary type="html">&lt;p&gt;You can use &lt;a href="https://developers.redhat.com/topics/kubernetes/operators"&gt;Kubernetes Operators&lt;/a&gt; to set up automatic resource management for services in your applications. In a &lt;a href="https://developers.redhat.com/blog/2020/12/16/create-a-kubernetes-operator-in-golang-to-automatically-manage-a-simple-stateful-application"&gt;previous article&lt;/a&gt;, I described this pattern and how to create an operator in the &lt;a href="https://developers.redhat.com/search?t=go"&gt;Go&lt;/a&gt; language. In this article, we will continue to explore the pattern, this time by creating a Kubernetes Operator in Go to keep a WordPress site up to date. I recommend reading or reviewing the earlier article before starting this one.&lt;/p&gt; &lt;h2&gt;Example prerequisites&lt;/h2&gt; &lt;p&gt;I use the following tools in this article's example:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Golang v1.15+&lt;/li&gt; &lt;li&gt;The &lt;a href="https://docs.openshift.com/container-platform/4.7/operators/operator_sdk/osdk-installing-cli.html"&gt;operator-sdk&lt;/a&gt; command-line interface, version 1 or higher&lt;/li&gt; &lt;li&gt;&lt;a href="https://minikube.sigs.k8s.io/docs/start/"&gt;minikube&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kubernetes.io/docs/tasks/tools/"&gt;kubectl&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In addition, set this environment variable in your terminal:&lt;/p&gt; &lt;pre&gt; $ export GO111MODULE="on"&lt;/pre&gt; &lt;p&gt;You can see the source code for this article on my &lt;a href="https://github.com/priyanka19-98/wordpress-operator-latest"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Set up the environment&lt;/h2&gt; &lt;p&gt;Before coding our automation, we need to set up the environment.&lt;/p&gt; &lt;p&gt;Enter the following command to generate &lt;a href="https://github.com/priyanka19-98/wordpress-operator-latest/commit/3beeff0758bf500b14a68e8e8e7a8d1bf90abb93"&gt;boilerplate code&lt;/a&gt; that accomplishes much of the setup:&lt;/p&gt; &lt;pre&gt; $ operator-sdk init --domain example.com --repo github.com/&lt;git user name&gt;/wordpress-operator &lt;/pre&gt; &lt;h3&gt;Creating the API and controller&lt;/h3&gt; &lt;p&gt;Enter the following to set up the environment for WordPress:&lt;/p&gt; &lt;pre&gt; $ operator-sdk create api --group wordpress --version v1 --kind Wordpress --resource --controller &lt;/pre&gt; &lt;p&gt;The command creates the following files:&lt;/p&gt; &lt;pre&gt; api/v1/wordpress_types.go controllers/wordpress_controller.go&lt;/pre&gt; &lt;h3&gt;Defining the API&lt;/h3&gt; &lt;p&gt;You can make changes to &lt;code&gt;api/v1/wordpress_types.go&lt;/code&gt; to include the attributes that you would like in the &lt;code&gt;WordpressSpec&lt;/code&gt; struct and &lt;code&gt;WordpressStatus&lt;/code&gt; status.&lt;/p&gt; &lt;p&gt;For the sake of simplicity, we will set only one requirement. It places a root password for database access in the &lt;code&gt;WordpressSpec&lt;/code&gt; struct:&lt;/p&gt; &lt;pre&gt; apiVersion: wordpress.example.com/v1 kind: Wordpress metadata: name: mysite spec: sqlRootPassword: plaintextpassword&lt;/pre&gt; &lt;p&gt;After you make the change in &lt;code&gt;api/v1/wordpress_types.go&lt;/code&gt;, run the following commands to ensure that the changes are reflected in the. custom resource definitions (CRDs) in the config directory:&lt;/p&gt; &lt;pre&gt; $ make generate $ make manifests &lt;/pre&gt; &lt;h2&gt;Implement the controller&lt;/h2&gt; &lt;p&gt;Now we can enter the Go code of the example to configure our controller. The WordPress controller struct is named &lt;code&gt;WordpressReconciler&lt;/code&gt; in the current &lt;code&gt;operator-sdk&lt;/code&gt;. In earlier versions, it was named &lt;code&gt;ReconcileWordpress&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Watching resources&lt;/h3&gt; &lt;p&gt;The boiler-plate code generated automatically creates a watch for the primary resource in the &lt;code&gt;controllers/wordpress_controller.go&lt;/code&gt; file, in the &lt;code&gt;SetupWithmanager&lt;/code&gt; function:&lt;/p&gt; &lt;pre&gt; For(&amp;v1.Wordpress{})&lt;/pre&gt; &lt;p&gt;The preceding line is equivalent to the following code in previous versions of the &lt;code&gt;operator-sdk&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; Watch(&amp;source.Kind{Type: &amp;v1.Wordpress{}}, &amp;handler.EnqueueRequestForObject{})&lt;/pre&gt; &lt;p&gt;The additional resources we watch are the service, the deployment, and the persistent volume claim (PVC):&lt;/p&gt; &lt;pre&gt; Owns(&amp;appsv1.Deployment{}). Owns(&amp;corev1.Service{}). Owns(&amp;corev1.PersistentVolumeClaim{})&lt;/pre&gt; &lt;p&gt;The preceding code is equivalent to the following code in previous versions of the &lt;code&gt;operator-sdk&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; Watch(&amp;source.Kind{Type: &amp;appsv1.Deployment{}}, &amp;handler.EnqueueRequestForOwner{ IsController: True, OwnerType: &amp;v1.Wordpress{}, }) Watch(&amp;source.Kind{Type: &amp;corev1.Service{}}, &amp;handler.EnqueueRequestForOwner{ IsController: true, OwnerType: &amp;v1.Wordpress{}, }) Watch(&amp;source.Kind{Type: &amp;corev1.PersistentVolumeClaim{}}, &amp;handler.EnqueueRequestForOwner{ IsController: true, OwnerType: &amp;v1.Wordpress{}, })&lt;/pre&gt; &lt;p&gt;To sum up, all the watches in the application can be written as:&lt;/p&gt; &lt;pre&gt; For(&amp;v1.Wordpress{}). Owns(&amp;appsv1.Deployment{}). Owns(&amp;corev1.Service{}). Owns(&amp;corev1.PersistentVolumeClaim{}). Complete(r)&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: To learn more about establishing watches in the &lt;code&gt;operator-sdk&lt;/code&gt;, refer to &lt;a href="https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/builder#example-Builder"&gt;Builder docs&lt;/a&gt; and &lt;a href="https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/controller"&gt;Controller docs&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;The reconcile loop&lt;/h3&gt; &lt;p&gt;I introduced the reconcile logic in my previous article. The new version of the &lt;code&gt;operator-sdk&lt;/code&gt; adds a few enhancements. It defines a few role-based access control (RBAC) rules by default:&lt;/p&gt; &lt;pre&gt; //+kubebuilder:rbac:groups=wordpress.example.com,resources=wordpresses,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=wordpress.example.com,resources=wordpresses/status,verbs=get;update;patch //+kubebuilder:rbac:groups=wordpress.example.com,resources=wordpresses/finalizers,verbs=update &lt;/pre&gt; &lt;p&gt;These are generated in the configuration directory when you run the &lt;code&gt;make manifests&lt;/code&gt; command.&lt;/p&gt; &lt;h3&gt;Logging&lt;/h3&gt; &lt;p&gt;The current &lt;code&gt;operator-sdk&lt;/code&gt; already defines a logger in the &lt;code&gt;WordpressReconciler&lt;/code&gt; struct. As a result, you don’t need to explicitly define your own logger.&lt;/p&gt; &lt;h3&gt;Adding the WordPress controller to the manager&lt;/h3&gt; &lt;p&gt;The current &lt;code&gt;operator-sdk&lt;/code&gt; adds the controller in &lt;code&gt;main.go&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; if err = (&amp;controllers.WordpressReconciler{ Client: mgr.GetClient(), Log: ctrl.Log.Withname(controllers”).Withname(“wordpress”), Scheme: mgr.GetScheme(), }).SetupWithManager(mgr); err!=nil{ // log error message os.Exit(1) } &lt;/pre&gt; &lt;p&gt;Note that this code is equivalent to the following code in previous versions of the &lt;code&gt;operator-sdk&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; if err := controllers.AddToManager(mgr); err != nil { log.Error(err, "") os.Exit(1) } &lt;/pre&gt; &lt;h2&gt;Run the Kubernetes Operator&lt;/h2&gt; &lt;p&gt;You need a Kubernetes cluster for this part of the example. Enter the following command :&lt;/p&gt; &lt;pre&gt; $ make install run &lt;/pre&gt; &lt;p&gt;Open another terminal window and apply the custom resource for WordPress:&lt;/p&gt; &lt;pre&gt; $ kubectl create -f config/samples/wordpress_v1_wordpress.yaml &lt;/pre&gt; &lt;p&gt;Your &lt;code&gt;config/samples/wordpress_v1_wordpress.yaml&lt;/code&gt; must look something like this:&lt;/p&gt; &lt;pre&gt; apiVersion: wordpress.example.com/v1 kind: Wordpress metadata: name: mysite spec: sqlrootpassword: "abc" &lt;/pre&gt; &lt;p&gt;Output from &lt;code&gt;make install run&lt;/code&gt; should be similar to the following:&lt;/p&gt; &lt;pre&gt; /home/pjiandan/go/src/wordpress-operator/bin/controller-gen "crd:trivialVersions=true,preserveUnknownFields=false" rbac:roleName=manager-role webhook paths="./..." output:crd:artifacts:config=config/crd/bases /home/pjiandan/go/src/wordpress-operator/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/wordpresses.wordpress.example.com created /home/pjiandan/go/src/wordpress-operator/bin/controller-gen "crd:trivialVersions=true,preserveUnknownFields=false" rbac:roleName=manager-role webhook paths="./..." output:crd:artifacts:config=config/crd/bases /home/pjiandan/go/src/wordpress-operator/bin/controller-gen object:headerFile="hack/boilerplate.go.txt" paths="./..." go fmt ./... go vet ./... go run ./main.go 2021-07-05T14:15:59.495+0530 INFO controller-runtime.metrics metrics server is starting to listen {"addr": ":8080"} 2021-07-05T14:15:59.496+0530 INFO setup starting manager 2021-07-05T14:15:59.497+0530 INFO controller-runtime.manager starting metrics server {"path": "/metrics"} 2021-07-05T14:15:59.497+0530 INFO controller-runtime.manager.controller.wordpress Starting EventSource {"reconciler group": "wordpress.example.com", "reconciler kind": "Wordpress", "source": "kind source: /, Kind="} 2021-07-05T14:15:59.598+0530 INFO controller-runtime.manager.controller.wordpress Starting EventSource {"reconciler group": "wordpress.example.com", "reconciler kind": "Wordpress", "source": "kind source: /, Kind="} 2021-07-05T14:15:59.698+0530 INFO controller-runtime.manager.controller.wordpress Starting EventSource {"reconciler group": "wordpress.example.com", "reconciler kind": "Wordpress", "source": "kind source: /, Kind="} 2021-07-05T14:15:59.799+0530 INFO controller-runtime.manager.controller.wordpress Starting EventSource {"reconciler group": "wordpress.example.com", "reconciler kind": "Wordpress", "source": "kind source: /, Kind="} 2021-07-05T14:15:59.900+0530 INFO controller-runtime.manager.controller.wordpress Starting Controller {"reconciler group": "wordpress.example.com", "reconciler kind": "Wordpress"} 2021-07-05T14:15:59.900+0530 INFO controller-runtime.manager.controller.wordpress Starting workers {"reconciler group": "wordpress.example.com", "reconciler kind": "Wordpress", "worker count": 1} 2021-07-05T14:15:59.900+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:15:59.900+0530 INFO controllers.Wordpress Creating a new PVC {"PVC.Namespace": "default", "PVC.Name": "mysql-pv-claim"} 2021-07-05T14:15:59.947+0530 INFO controllers.Wordpress Creating a new Deployment {"Deployment.Namespace": "default", "Deployment.Name": "wordpress-mysql"} 2021-07-05T14:15:59.969+0530 INFO controllers.Wordpress Creating a new Service {"Service.Namespace": "default", "Service.Name": "wordpress-mysql"} 2021-07-05T14:15:59.977+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:15:59.978+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:15:59.978+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:15:59.998+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:15:59.998+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:16:00.044+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:16:00.044+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:16:00.058+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:16:00.059+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:17:09.986+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:17:09.987+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:17:24.988+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:17:24.988+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:18:09.993+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:18:09.993+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:18:14.993+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:18:14.994+0530 INFO controllers.Wordpress MySQL isn't running, waiting for 5s 2021-07-05T14:19:10.278+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:19:10.278+0530 INFO controllers.Wordpress Creating a new PVC {"PVC.Namespace": "default", "PVC.Name": "wp-pv-claim"} 2021-07-05T14:19:10.292+0530 INFO controllers.Wordpress Creating a new Deployment {"Deployment.Namespace": "default", "Deployment.Name": "wordpress"} 2021-07-05T14:19:10.309+0530 INFO controllers.Wordpress Creating a new Service {"Service.Namespace": "default", "Service.Name": "wordpress"} 2021-07-05T14:19:10.341+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:19:10.355+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:19:10.361+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:19:10.371+0530 INFO controllers.Wordpress Reconciling Wordpress 2021-07-05T14:19:15.001+0530 INFO controllers.Wordpress Reconciling Wordpress &lt;/pre&gt; &lt;p&gt;From the second terminal window, check for the resources created as follows:&lt;/p&gt; &lt;pre&gt; [pjiandan@localhost wordpress-operator]$ kubectl get all NAME READY STATUS RESTARTS AGE pod/wordpress-7446b985d9-vc7w2 1/1 Running 0 128m pod/wordpress-mysql-5cd8987844-4p6jp 1/1 Running 0 128m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 3h58m service/wordpress NodePort 10.110.12.200 &lt;none&gt; 80:32324/TCP 128m service/wordpress-mysql ClusterIP None &lt;none&gt; 3306/TCP 128m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/wordpress 1/1 1 1 128m deployment.apps/wordpress-mysql 1/1 1 1 128m NAME DESIRED CURRENT READY AGE replicaset.apps/wordpress-7446b985d9 1 1 1 128m replicaset.apps/wordpress-mysql-5cd8987844 1 1 1 128m &lt;/pre&gt; &lt;p&gt;Next, run the following command to return the IP address for the WordPress service:&lt;/p&gt; &lt;pre&gt; [pjiandan@localhost wordpress-operator]$ minikube service wordpress --url http://192.168.99.143:32324 &lt;/pre&gt; &lt;h2&gt;Verify that WordPress is running&lt;/h2&gt; &lt;p&gt;Open the URL in your browser to verify that WordPress is running, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/go_wp.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/go_wp.png?itok=bwV0o7ML" width="359" height="538" alt="A running instance of WordPress shows that the Operator was successful." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt; &lt;/span&gt; &lt;/span&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A running instance of WordPress. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Visit my &lt;a href="https://github.com/priyanka19-98/wordpress-operator-latest"&gt;GitHub repository&lt;/a&gt; to see all of the code used in this example.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/04/managing-stateful-applications-kubernetes-operators-golang" title="Managing stateful applications with Kubernetes Operators in Golang"&gt;Managing stateful applications with Kubernetes Operators in Golang&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/o1kPvgHNpS4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Priyanka Jiandani</dc:creator><dc:date>2021-08-04T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/04/managing-stateful-applications-kubernetes-operators-golang</feedburner:origLink></entry><entry><title type="html">Integrating DMN Validation on BC, Maven, Kogito, …and beyond!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lhb1FzA2s3o/integrating-dmn-validation-on-bc-maven-kogito-and-beyond.html" /><author><name>Matteo Mortari</name></author><id>http://feeds.athico.com/~r/droolsatom/~3/07dbBWd12do/integrating-dmn-validation-on-bc-maven-kogito-and-beyond.html</id><updated>2021-08-03T11:51:07Z</updated><content type="html">In this post I want to highlight all the integrations of the kie-dmn-validation module on several platforms. WHAT IS IT? In a , we have seen how the Drools DMN validation module was integrated specifically on the Kogito platform. As a short review, the kie-dmn-validation module offer several features for: * validation of model against specification XSDs * static validation of DMN file * e.g.: pre-compilation phase semantic validations (duplicate names, missing decision logic, etc.) * fun-fact: static validation is performed with… Drools rules! * compilation phase checks * decision tables static analysis * implements  * semantic checks * Hit Policy recommender * Experimental features, such as the MC/DC test case generator The pre-compilation phase, where semantic validations are performed statically by introspecting deserialised DMN models, make use of to ensure the conformance requirements from the  itself are respected in the DMN model provided by the user. INTEGRATION ON KIE V7: BUSINESS CENTRAL AND MAVEN (KJAR) Speaking of Kie (v7) platforms, the DMN Validation is performed out of the box: * When editing a DMN Model on Business Central the Validation is performed when clicking the Save button, and when the full project is being built. * When building the KJAR-based project with Maven the Validation is performed by the kie-maven-plugin for any DMN model knowledge asset found in the KJAR project automatically There is some fine-print about out of the box DMN Validation enabled features, on the Kie v7 platform: kie-maven-pluginDMN Editor on BCVALIDATE_SCHEMA compliance with DMN xmlActive by defaultSkipped (always skipped)VALIDATE_MODEL semantic validationActive by defaultAlways performedVALIDATE_COMPILATION consistency during compilationActive by defaultAlways performed*ANALYZE_DECISION_TABLE decision table static analysisActive by defaultAlways performed * except for DMN-&gt;Java integration checks () The table summarises how Business Central and the kie-maven-plugin use validation features of the DMN Validation module. There is no option to customize which features to use on BC. When using a Maven build of a KJAR project with the kie-maven-plugin, you can customize which features to use; you can reference to referring to "" and the validateDMN option of the kie-maven-plugin configuration. INTEGRATION ON KOGITO As mentioned in the , the kie-dmn-validation module is integrated out of the box on the Kogito platform, during the code generation phase. All DMN Validation features are enabled by default. You can customize with a configuration option whether to disable this validation integration; you can reference to referring to "" and the kogito.decisions.validation option of the Kogito application configuration. INTEGRATION ON JIT EXECUTOR The Just-in-Time (JIT) executor is an experimental module based on Kogito, Quarkus and Native Image compilation; it is meant as a foundational tool to support development-time of DMN models and other knowledge assets. The JIT Executor provides integration of the kie-dmn-validation to perform all the DMN validation by means of an API; you can reference . You might be interested to know the JIT Executor is the underlying engine empowering the local DMN runner, described in . CONCLUSIONS As we have seen in this article, not only the kie-dmn-validation module provides several helpful features to validate DMN model during build-time, such as semantic validation and Decision Table static analysis! It is also a set of comprehensive capabilities which we have integrated for you on several platforms, including Kie v7 KJAR maven build, Business Central, Kogito and Kogito toolings! Have you tried it? Let us know your feedback in the comments below! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lhb1FzA2s3o" height="1" width="1" alt=""/&gt;</content><dc:creator>Matteo Mortari</dc:creator><feedburner:origLink>http://feeds.athico.com/~r/droolsatom/~3/07dbBWd12do/integrating-dmn-validation-on-bc-maven-kogito-and-beyond.html</feedburner:origLink></entry><entry><title>How to patch modules in Red Hat Enterprise Linux</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rIvhVOaOwzQ/how-patch-modules-red-hat-enterprise-linux" /><author><name>Petr Pisar</name></author><id>ead51897-4c95-47c9-9b11-cf8ff1f05c6c</id><updated>2021-08-03T07:00:00Z</updated><published>2021-08-03T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL), in version 8, introduced &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#introduction-to-modules_using-appstream"&gt;modules&lt;/a&gt; as a higher-level concept for packaging software stacks. Modules enable new features such as adding alternative versions of stacks, called &lt;em&gt;streams&lt;/em&gt;. That's great, but what if you want to patch a stream? Is it possible? It is. Is it more difficult than patching non-modular software? Slightly. This article shows you how to patch a module stream while avoiding the invisible package problem.&lt;/p&gt; &lt;h2&gt;Patching a module in RHEL&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux is open source. That means you can take the code sources, change them, recompile them, and use or redistribute the modified software. As an example, we can change the HTTPD web server to report a different server name in the HTTP response headers.&lt;/p&gt; &lt;p&gt;To get started, install an &lt;code&gt;httpd&lt;/code&gt; RPM package, start the HTTPD server, and check the server name. I have highlighted the relevant lines from the output in bold:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# yum install httpd Last metadata expiration check: 0:03:40 ago on Fri 16 Jul 2021 12:51:49 PM CEST. Dependencies resolved. ========================================================================================== Package Arch Version Repository Size ========================================================================================== Installing: &lt;strong&gt;httpd x86_64 2.4.37-40.module+el8.5.0+11022+1c90597b&lt;/strong&gt; rhel-8.5.0-appstream 1.4 M Installing dependencies: httpd-filesystem noarch 2.4.37-40.module+el8.5.0+11022+1c90597b rhel-8.5.0-appstream 39 k httpd-tools x86_64 2.4.37-40.module+el8.5.0+11022+1c90597b rhel-8.5.0-appstream 106 k mod_http2 x86_64 1.15.7-3.module+el8.4.0+8625+d397f3da pulp-appstream 154 k redhat-logos-httpd noarch 84.5-1.el8 rhel-8.5.0-baseos 29 k Enabling module streams: &lt;strong&gt;httpd 2.4&lt;/strong&gt; Transaction Summary ========================================================================================== Install 5 Packages Total download size: 1.7 M Installed size: 4.9 M Is this ok [y/N]: y […] Complete! # systemctl start httpd $ wget --no-proxy -S -O /dev/null http://localhost/ --2021-07-16 12:58:54-- http://localhost/ Resolving localhost (localhost)... ::1, 127.0.0.1 Connecting to localhost (localhost)|::1|:80... connected. HTTP request sent, awaiting response... HTTP/1.1 403 Forbidden Date: Fri, 16 Jul 2021 10:58:54 GMT &lt;strong&gt;Server: Apache/2.4.37 (Red Hat Enterprise Linux)&lt;/strong&gt; Last-Modified: Mon, 12 Jul 2021 19:36:32 GMT ETag: "133f-5c6f23d09f000" Accept-Ranges: bytes Content-Length: 4927 Keep-Alive: timeout=5, max=100 Connection: Keep-Alive Content-Type: text/html; charset=UTF-8 2021-07-16 12:58:54 ERROR 403: Forbidden.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that the &lt;code&gt;httpd-2.4.37-40.module+el8.5.0+11022+1c90597b&lt;/code&gt; RPM package was installed from the &lt;code&gt;httpd:2.4&lt;/code&gt; module stream and that the server reports &lt;code&gt;Apache/2.4.37 (Red Hat Enterprise Linux)&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Our quest is to patch the module to report &lt;code&gt;My Linux&lt;/code&gt; instead.&lt;/p&gt; &lt;h2&gt;Step 1: Build a new package&lt;/h2&gt; &lt;p&gt;First, obtain the source RPM package, &lt;code&gt;httpd-2.4.37-40.module+el8.5.0+11022+1c90597b.src.rpm&lt;/code&gt;, which corresponds to our example. Unpack it and apply the following patch to a specification file, as explained in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/packaging_and_distributing_software/index#patching-software_preparing-software-for-rpm-packaging"&gt;Red Hat documentation&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- a/httpd.spec +++ b/httpd.spec @@ -13,7 +13,7 @@ Summary: Apache HTTP Server Name: httpd Version: 2.4.37 -Release: 40%{?dist} +Release: 41%{?dist} URL: https://httpd.apache.org/ Source0: https://www.apache.org/dist/httpd/httpd-%{version}.tar.bz2 Source2: httpd.logrotate @@ -370,7 +370,7 @@ interface for storing and accessing per-user session data. %patch211 -p1 -b .CVE-2020-11984 # Patch in the vendor string -sed -i '/^#define PLATFORM/s/Unix/%{vstring}/' os/unix/os.h +sed -i '/^#define PLATFORM/s/Unix/My Linux/' os/unix/os.h sed -i 's/@RELEASE@/%{release}/' server/core.c # Prevent use of setcap in "install-suexec-caps" target. @@ -870,6 +870,9 @@ rm -rf $RPM_BUILD_ROOT %{_rpmconfigdir}/macros.d/macros.httpd %changelog +* Wed Jun 23 2021 Petr Pisar &lt;ppisar@redhat.com&gt; - 2.4.37-41 +- Modified server platform + * Fri May 14 2021 Lubos Uhliarik &lt;luhliari@redhat.com&gt; - 2.4.37-40 - Resolves: #1952557 - mod_proxy_wstunnel.html is a malformed XML - Resolves: #1937334 - SSLProtocol with based virtual hosts&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/packaging_and_distributing_software/index#building-binary-rpms_building-rpms"&gt;build the modified package&lt;/a&gt; with a &lt;code&gt;rpmbuild&lt;/code&gt; tool. This results in the following binary packages:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ls httpd-2.4.37-41.el8.x86_64.rpm mod_ldap-2.4.37-41.el8.x86_64.rpm httpd-debuginfo-2.4.37-41.el8.x86_64.rpm mod_ldap-debuginfo-2.4.37-41.el8.x86_64.rpm httpd-debugsource-2.4.37-41.el8.x86_64.rpm mod_proxy_html-2.4.37-41.el8.x86_64.rpm httpd-devel-2.4.37-41.el8.x86_64.rpm mod_proxy_html-debuginfo-2.4.37-41.el8.x86_64.rpm httpd-filesystem-2.4.37-41.el8.noarch.rpm mod_session-2.4.37-41.el8.x86_64.rpm httpd-manual-2.4.37-41.el8.noarch.rpm mod_session-debuginfo-2.4.37-41.el8.x86_64.rpm httpd-tools-2.4.37-41.el8.x86_64.rpm mod_ssl-2.4.37-41.el8.x86_64.rpm httpd-tools-debuginfo-2.4.37-41.el8.x86_64.rpm mod_ssl-debuginfo-2.4.37-41.el8.x86_64.rpm &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 2: Create a nonmodular repository&lt;/h2&gt; &lt;p&gt;Next, turn the directory into a YUM repository. Let's say that the repository is located in the working directory &lt;code&gt;/root/repos/myhttpd&lt;/code&gt;, so all write operations there must be performed by a superuser:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# createrepo_c . Directory walk started Directory walk done - 16 packages Temporary output repo path: ./.repodata/ Preparing sqlite DBs Pool started (with 5 workers) Pool finished&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Register the repository to YUM under the name &lt;code&gt;myhttpd&lt;/code&gt; by creating the &lt;code&gt;/etc/yum.repos.d/devel.repo&lt;/code&gt; file with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[myhttpd] name=myhttpd packages baseurl=file:///root/repos/myhttpd/ enabled=1 gpgcheck=0&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The invisible package problem&lt;/h3&gt; &lt;p&gt;Next, let's try to update the system to install the patched package:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# yum upgrade myhttpd packages 2.9 MB/s | 3.0 kB 00:00 myhttpd packages 2.5 MB/s | 25 kB 00:00 Dependencies resolved. Nothing to do. Complete!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It doesn't work! YUM cannot see your new &lt;code&gt;httpd-2.4.37-41.el8.x86_64&lt;/code&gt; package. Check which packages YUM sees:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ repoquery httpd Last metadata expiration check: 0:06:19 ago on Fri 16 Jul 2021 01:31:22 PM CEST. httpd-0:2.4.37-10.module+el8+2764+7127e69e.x86_64 httpd-0:2.4.37-11.module+el8.0.0+2969+90015743.x86_64 httpd-0:2.4.37-12.module+el8.0.0+4096+eb40e6da.x86_64 httpd-0:2.4.37-16.module+el8.1.0+4134+e6bad0ed.x86_64 httpd-0:2.4.37-21.module+el8.2.0+5008+cca404a3.x86_64 httpd-0:2.4.37-30.module+el8.3.0+7001+0766b9e7.x86_64 httpd-0:2.4.37-39.module+el8.4.0+9658+b87b2deb.x86_64 httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The new package isn't there, but why? Because packages belonging to an active module stream take precedence over other packages of the same name. This issue is sometimes known as the invisible package problem. We'll explore it in the next section.&lt;/p&gt; &lt;h3&gt;Theory of modules&lt;/h3&gt; &lt;p&gt;To resolve the invisible package problem, you need to understand how modules work.&lt;/p&gt; &lt;p&gt;Modules are organized into streams (examples include &lt;code&gt;httpd:2.4&lt;/code&gt;, &lt;code&gt;perl:5.24&lt;/code&gt;, and &lt;code&gt;perl:5.30&lt;/code&gt;; also see the &lt;code&gt;yum module list&lt;/code&gt; command output). Each stream consists of a series of module versions (such as &lt;code&gt;httpd:2.4:8040020210127115317&lt;/code&gt; and &lt;code&gt;httpd:2.4:8050020210517115912&lt;/code&gt;) and each module version lists RPM packages belonging to it (see the &lt;code&gt;Artifacts&lt;/code&gt; section in the output from &lt;code&gt;yum module info httpd:2.4&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;A module stream is &lt;em&gt;active&lt;/em&gt; if the developer enables it explicitly, or if it is the default and has not been explicitly disabled. All packages belonging to the active stream are visible to YUM. All other packages of the same name, including packages not belonging to any module, are invisible.&lt;/p&gt; &lt;p&gt;Correspondingly, when a module stream is &lt;em&gt;not active&lt;/em&gt; (is disabled or is nondefault), its packages are invisible, while nonmodular packages with the same name are kept visible.&lt;/p&gt; &lt;p&gt;In a typical Red Hat Enterprise Linux distribution, you can observe changes in visibility as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Enable the &lt;code&gt;perl:5.24&lt;/code&gt; stream by running &lt;code&gt;yum enable perl:5.24&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;List the Perl packages in the repository by entering &lt;code&gt;repoquery perl&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Reset the stream through &lt;code&gt;yum module reset perl&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Enable &lt;code&gt;perl:5.30&lt;/code&gt; in a similar way.&lt;/li&gt; &lt;li&gt;List the packages again and view the differences from the previous listing.&lt;/li&gt; &lt;li&gt;Reset the stream to &lt;code&gt;perl:5.26&lt;/code&gt;, or whatever the default was on your system.&lt;/li&gt; &lt;li&gt;List the packages again.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;After each change, YUM lists different &lt;code&gt;perl&lt;/code&gt; packages.&lt;/p&gt; &lt;h3&gt;Solving the invisible package problem&lt;/h3&gt; &lt;p&gt;In our example, the new &lt;code&gt;httpd-2.4.37-41.el8.x86_64&lt;/code&gt; is currently prevented from being visible. The &lt;code&gt;httpd:2.4&lt;/code&gt; module stream is active and lists an &lt;code&gt;httpd&lt;/code&gt; package:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ yum module info httpd:2.4 […] Name : httpd Stream : 2.4 [d][e][a] Version : 8050020210517115912 Context : b4937e53 Architecture : x86_64 Profiles : common [d], devel, minimal Default profiles : common Repo : rhel-8.5.0-appstream Summary : Apache HTTP Server Description : Apache httpd is a powerful, efficient, and extensible HTTP server. Requires : platform:[el8] Artifacts : httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.src : &lt;strong&gt;httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64&lt;/strong&gt; […]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To patch the module, you need to define a new &lt;code&gt;httpd:2.4&lt;/code&gt; module version, list the new &lt;code&gt;httpd-2.4.37-41.el8.x86_64&lt;/code&gt; package there, and add the new module version definition to the repository. After you've done that, YUM will recognize the new package as belonging to the &lt;code&gt;httpd:2.4&lt;/code&gt; stream, and you can continue to the next step.&lt;/p&gt; &lt;h2&gt;Step 3: Make the repository modular&lt;/h2&gt; &lt;p&gt;Now comes a step specific to modules: Changing a nonmodular repository into a modular one. Copy the &lt;code&gt;httpd:2.4:8050020210517115912:b4937e53:x86_64&lt;/code&gt; module definition from the original repository to the &lt;code&gt;modules.yaml&lt;/code&gt; file in the directory with the new package:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# zcat /var/cache/dnf/rhel-8.5.0-appstream-801b3acbf7fb96cf/repodata/7642b0bd7a55141335285144eb537352c85f336de8187ad14aa40b0dbf532463-modules.yaml.gz &gt; modules.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I took the file from a local YUM cache. But you can also find files with names matching &lt;code&gt;*-modules.yaml*&lt;/code&gt; on repository mirrors.&lt;/p&gt; &lt;p&gt;Now, locate the module build definition inside the file and delete everything else:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- document: modulemd version: 2 data: name: httpd stream: "2.4" version: 8050020210517115912 context: b4937e53 arch: x86_64 […] artifacts: rpms: - httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.src - httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-debuginfo-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-debugsource-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-devel-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-filesystem-0:2.4.37-40.module+el8.5.0+11022+1c90597b.noarch - httpd-manual-0:2.4.37-40.module+el8.5.0+11022+1c90597b.noarch - httpd-tools-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-tools-debuginfo-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_http2-0:1.15.7-3.module+el8.4.0+8625+d397f3da.src - mod_http2-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_http2-debuginfo-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_http2-debugsource-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_ldap-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_ldap-debuginfo-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_md-1:2.0.8-8.module+el8.3.0+6814+67d1e611.src - mod_md-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_md-debuginfo-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_md-debugsource-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_proxy_html-1:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_proxy_html-debuginfo-1:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_session-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_session-debuginfo-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_ssl-1:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_ssl-debuginfo-1:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Update the RPM package list in the &lt;code&gt;/data/artifacts/rpms&lt;/code&gt; YAML node to match your new RPM builds:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# sed -i -e 's/-40\.module+el8\.5\.0+11022+1c90597b\./-41.el8./' modules.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Increment the module build version; for example, from &lt;code&gt;8050020210517115912&lt;/code&gt; to &lt;code&gt;8050020210517115913&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- document: modulemd version: 2 data: name: httpd stream: "2.4" version: &lt;strong&gt;8050020210517115913&lt;/strong&gt; context: b4937e53 arch: x86_64 [...] artifacts: rpms: - httpd-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.src - httpd-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-debuginfo-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-debugsource-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-devel-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-filesystem-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.noarch - httpd-manual-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.noarch - httpd-tools-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-tools-debuginfo-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_http2-0:1.15.7-3.module+el8.4.0+8625+d397f3da.src - mod_http2-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_http2-debuginfo-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_http2-debugsource-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_ldap-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_ldap-debuginfo-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_md-1:2.0.8-8.module+el8.3.0+6814+67d1e611.src - mod_md-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_md-debuginfo-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_md-debugsource-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_proxy_html-1:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_proxy_html-debuginfo-1:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_session-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_session-debuginfo-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_ssl-1:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_ssl-debuginfo-1:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the module lists other packages, you can delete them.&lt;/p&gt; &lt;p&gt;Finally, regenerate the repository metadata so that it picks up the new module definition from the &lt;code&gt;modules.yaml&lt;/code&gt; file in the local directory:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# createrepo_c . Directory walk started Directory walk done - 16 packages Temporary output repo path: ./.repodata/ Preparing sqlite DBs Pool started (with 5 workers) Pool finished&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check that the module definition was imported under the known &lt;code&gt;*-module.yaml.*&lt;/code&gt; filename:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ls repodata/ 733d406732770bce66f8b790a92e933559903e7c3360a1bada3de366c130fc7c-other.sqlite.bz2 a99faa19e499bec174b3c3c682d150715fc66015da00511aa9f78691a3670708-other.xml.gz aaffc762d36b0389d602c9c37db74242cae8d37ea89d3aa3e4ca2b4cc4099b0d-primary.sqlite.bz2 d10db6feb91cc5f185218367162cdbab49780343a82efe23e6d8c0e14f4effcb-filelists.xml.gz e51d17bf9000bd130f99edd8ff2923977c8c74a0b5829116e36299fb46a440e9-primary.xml.gz f98a57f75a9fb84f1ce0313ee22e435eebb6134a28f7568ad8b8b4e14be38285-filelists.sqlite.bz2 &lt;strong&gt;ff2b17e5a515266023ccc983a8cf12401ae4d2c2049683e76ad09f6b5cea48ba-modules.yaml.gz&lt;/strong&gt; repomd.xml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can delete the &lt;code&gt;./modules.yaml&lt;/code&gt; file. You don't need it anymore. The repository is now modular.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Importing modular metadata from a &lt;code&gt;modules.yaml&lt;/code&gt; file is a new feature of &lt;code&gt;createrepo-c-0.16.2&lt;/code&gt;. If you have an older version, you need to use the &lt;code&gt;modifyrepo_c&lt;/code&gt; tool after running &lt;code&gt;createrepo_c&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Step 4: Install the package from the modular repository&lt;/h2&gt; &lt;p&gt;We are nearly done. Try updating the system again:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# yum upgrade myhttpd packages 1.4 MB/s | 26 kB 00:00 Dependencies resolved. ========================================================================================== Package Architecture Version Repository Size ========================================================================================== Upgrading: httpd x86_64 2.4.37-41.el8 myhttpd 1.4 M httpd-filesystem noarch 2.4.37-41.el8 myhttpd 37 k httpd-tools x86_64 2.4.37-41.el8 myhttpd 104 k Transaction Summary ========================================================================================== Upgrade 3 Packages Total size: 1.5 M Is this ok [y/N]: y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And that's it. It works. Hooray!&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If YUM did not refresh the repository, it might be because you performed the steps too quickly. Clean up the cache with &lt;code&gt;rm -rf /var/cache/dnf/myhttpd*&lt;/code&gt; and try again.&lt;/p&gt; &lt;h2&gt;Step 5: Verify the patched package&lt;/h2&gt; &lt;p&gt;Finally, you can check the &lt;code&gt;Server&lt;/code&gt; header that the server returns:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ wget --no-proxy -S -O /dev/null http://localhost/ --2021-07-16 15:15:56-- http://localhost/ Resolving localhost (localhost)... ::1, 127.0.0.1 Connecting to localhost (localhost)|::1|:80... connected. HTTP request sent, awaiting response... HTTP/1.1 403 Forbidden Date: Fri, 16 Jul 2021 13:15:56 GMT &lt;strong&gt;Server: Apache/2.4.37 (My Linux)&lt;/strong&gt; Last-Modified: Mon, 12 Jul 2021 19:36:32 GMT ETag: "133f-5c6f23d09f000" Accept-Ranges: bytes Content-Length: 4927 Keep-Alive: timeout=5, max=100 Connection: Keep-Alive Content-Type: text/html; charset=UTF-8 2021-07-16 15:15:56 ERROR 403: Forbidden.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The line &lt;code&gt;Server: Apache/2.4.37 (My Linux)&lt;/code&gt; shows that the server is running your patched module.&lt;/p&gt; &lt;h2&gt;Versioning patched modules and packages&lt;/h2&gt; &lt;p&gt;What version should you use for the patched modules? Currently, it doesn't matter much because YUM merges all module versions of a single stream together. But I recommend incrementing the last digit, as we did in our example. The module version is basically a timestamp. Incrementing the last digit means moving a second ahead. It's improbable that Red Hat would release two module versions with one-second delays. Thus, when Red Hat releases a new version, it's recognized as more recent than your version, and replaces your version.&lt;/p&gt; &lt;p&gt;The version number could matter if you want to change other modular metadata, such as modular dependencies. Then the highest module version wins.&lt;/p&gt; &lt;p&gt;What about the RPM version string? Inside a stream, a standard RPM epoch-version-release comparison is used to update a modular package to another modular package. If Red Hat released a new module update, the &lt;code&gt;httpd&lt;/code&gt; package would be called something like &lt;code&gt;httpd-0:2.4.37-41.module+el8.5.0+11022+1c90597b&lt;/code&gt;. That's fine because that would be a higher RPM version string than yours and the new update would win:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ rpmdev-vercmp 0:2.4.37-41.el8 0:2.4.37-41.module+el8.5.0+11022+1c90597b 0:2.4.37-41.el8 &lt; 0:2.4.37-41.module+el8.5.0+11022+1c90597b&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want your RPM package to win over future Red Hat updates, choose a reasonably high release number. The process is the same as what would you do in a nonmodular scenario.&lt;/p&gt; &lt;h2&gt;Special situations and warnings&lt;/h2&gt; &lt;p&gt;Sometimes there are multiple modules with the same version but a different context value. What does the context mean? Which context should you use? Can you change it?&lt;/p&gt; &lt;p&gt;The context distinguishes modules that were built from the same sources but for different environments. For instance, the &lt;code&gt;perl-DBI:1.641&lt;/code&gt; modules found in Red Hat Enterprise Linux 8.3 are built three times for three different Perl versions, so there are three different contexts of them. When you patch a module, don't change the context. Copying the old value is the safest approach.&lt;/p&gt; &lt;p&gt;I'll conclude by listing a few shortcuts that are not recommended for patching modules:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Installing from a local file. You can use &lt;code&gt;yum upgrade ./httpd-*.rpm&lt;/code&gt;, but the results won't last long. A package installed like that won't be recognized as belonging to any module and could be expelled from future YUM transactions, resulting in a dependency conflict on an RPM level. Also, having a package outside a repository makes it difficult to deploy to multiple machines or reinstall the package.&lt;/li&gt; &lt;li&gt;Adding a &lt;code&gt;module_hotfixes=true&lt;/code&gt; statement to a YUM configuration file for a nonmodular repository. While this technique works as a last resort for overriding any modular content, the hammer is too big for the nail. It does not play nicely when multiple module streams provide the same package, or if all the streams are disabled.&lt;/li&gt; &lt;li&gt;Omitting the zero epoch from an artifacts list in the module definition. Don't do it. YUM won't understand it. If your RPM package has no epoch number, write &lt;code&gt;0&lt;/code&gt;. You can use &lt;code&gt;rpm -q --qf '%{NAME}-%{EPOCHNUM}:%{VERSION}-%{RELEASE}.%{ARCH}\n' -p httpd-2.4.37-41.el8.x86_64.rpm&lt;/code&gt; to obtain the right value: &lt;code&gt;httpd-&lt;strong&gt;0:&lt;/strong&gt;2.4.37-41.el8.x86_64&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;References&lt;/h2&gt; &lt;p&gt;I recommend reading the &lt;a href="https://github.com/fedora-modularity/libmodulemd/blob/main/yaml_specs/modulemd_stream_v2.yaml"&gt;module definition format&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/03/how-patch-modules-red-hat-enterprise-linux" title="How to patch modules in Red Hat Enterprise Linux"&gt;How to patch modules in Red Hat Enterprise Linux&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rIvhVOaOwzQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Petr Pisar</dc:creator><dc:date>2021-08-03T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/03/how-patch-modules-red-hat-enterprise-linux</feedburner:origLink></entry><entry><title>What's new in the Red Hat OpenShift 4.8 console</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tM_WYj5RYNs/whats-new-red-hat-openshift-48-console" /><author><name>Serena Chechile Nichols</name></author><id>3ee6bb32-47bf-4136-b520-34b17b590fe0</id><updated>2021-08-02T07:00:00Z</updated><published>2021-08-02T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift"&gt;Red Hat OpenShift 4.8&lt;/a&gt; brings many exciting new capabilities for developers. This article focuses on what’s new in the OpenShift console, including devfiles and certified Helm charts in the developer catalog, the ability to drag and drop Spring or Quarkus JARs directly from your desktop, a streamlined developer experience for building, deploying, and scaling cloud-native applications in hybrid-cloud environments, and much more. Developers will also find enhanced features for &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;, &lt;a href="https://developers.redhat.com/courses/middleware/openshift-pipelines"&gt;Red Hat OpenShift Pipelines&lt;/a&gt;, &lt;a href="https://docs.openshift.com/container-platform/4.7/cicd/gitops/understanding-openshift-gitops.html"&gt;Red Hat OpenShift GitOps&lt;/a&gt;, and more.&lt;/p&gt; &lt;h2&gt;Dragging and dropping Spring and Quarkus apps&lt;/h2&gt; &lt;p&gt;Developers can drag and drop their fat JAR files for Spring and Quarkus apps directly from their desktop into the console topology view. The system does the rest of the work to deploy the applications on OpenShift for quick and easy testing. The &lt;strong&gt;Upload JAR&lt;/strong&gt; feature lets you quickly and easily test applications before pushing them to Git, without having to build a container image. This new feature expands how you can code and test locally using a command-line interface (odo) and IDE extensions in VS Code and IntelliJ for OpenShift Connector, &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As a console user, you can still check your code into your Git repository and use the &lt;strong&gt;Import from git&lt;/strong&gt; feature in the console. You can also build a container image, push it to a registry such as &lt;a href="https://quay.io/"&gt;Quay&lt;/a&gt;, and use the &lt;strong&gt;Deploy image&lt;/strong&gt; feature from the console.&lt;/p&gt; &lt;h2&gt;Getting started&lt;/h2&gt; &lt;p&gt;To improve developer onboarding, we have a new &lt;strong&gt;Getting started resources&lt;/strong&gt; card on the &lt;strong&gt;Add&lt;/strong&gt; page, as shown in Figure 1. This card provides resources to create applications using samples, build with guided documentation, and explore new developer features.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/addpage.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/addpage.png?itok=ulhpK7Tc" width="600" height="375" alt="There is a new "Getting started resources" card on the console's Add page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The "Getting started resources" card on the Add page. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Developer catalog&lt;/h2&gt; &lt;p&gt;OpenShift 4.8 makes &lt;a href="https://www.redhat.com/en/blog/red-hat-openshift-certification-extends-support-kubernetes-native-technologies-helm"&gt;certified Helm charts&lt;/a&gt; available from the developer catalog. These charts are provided by our partners and ensure the best available integration and experience on OpenShift. A badge in the developer catalog makes it easy to identify certified Helm charts, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/helmchart.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/helmchart.png?itok=30UfzOcV" width="600" height="375" alt="Certified Helm charts from partners are now in the developer catalog." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Certified Helm charts in the developer catalog. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In many of the discussions we are having with developers, one of the biggest pain points is around configuration, settings, and managing developer environments. Developers want to advance quickly to building, running, and debugging projects. &lt;a href="https://docs.devfile.io/devfile/2.1.0/user-guide/index.html"&gt;Devfiles&lt;/a&gt; offer exactly that capability.&lt;/p&gt; &lt;p&gt;In the newest version of OpenShift, you can now find devfiles in the developer catalog, as shown in Figure 3. You can use these to bootstrap a new project for a particular language or framework. Each devfile provides sample projects for quick use.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/devfiles.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/devfiles.png?itok=jGYNIHAm" width="600" height="367" alt="Devfiles, which enable fast application setup, are available in the developer catalog." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Devfiles in the developer catalog. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Devfiles are configuration files that set up a cloud-native project with the required information to build, run, debug, and more. Devfiles are already used in odo and CodeReady Workspaces. Check out the article &lt;a href="https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0"&gt;Developing your own custom devfiles for odo 2.0&lt;/a&gt; to learn more about creating custom devfiles.&lt;/p&gt; &lt;h2&gt;OpenShift Serverless&lt;/h2&gt; &lt;p&gt;The popular serverless method of running applications benefits from several enhancements in OpenShift 4.8.&lt;/p&gt; &lt;h3&gt;Make Serverless&lt;/h3&gt; &lt;p&gt;The &lt;strong&gt;Make Serverless&lt;/strong&gt; action, which is in tech preview, creates a new serverless deployment next to your existing deployment. Other configurations, including the traffic pattern, can be modified in the form. Figure 4 shows the new &lt;strong&gt;Make Serverless&lt;/strong&gt; action.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/serverless.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/serverless.gif" width="423" height="264" alt="This video shows how to use the Make Serverless action to create a serverless deployment." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Make Serverless procedure. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Cloud functions&lt;/h3&gt; &lt;p&gt;The topology view in the developer console now lets you visualize cloud functions. Event sources can specify cloud functions as their sinks.&lt;/p&gt; &lt;h3&gt;Advanced scaling options for Knative services&lt;/h3&gt; &lt;p&gt;We now have enhanced scaling options for Knative services. The &lt;strong&gt;Concurrency utilization&lt;/strong&gt; feature allows you to set the percentage of concurrent requests that have to be active before scaling up. &lt;strong&gt;Autoscale window&lt;/strong&gt; allows you to set the amount of time to look back while making autoscaling decisions. The service is scaled down to zero if no requests are received in that time period.&lt;/p&gt; &lt;h2&gt;OpenShift Pipelines&lt;/h2&gt; &lt;p&gt;The OpenShift developer console now has feature parity with Tekton within the Pipeline Builder and other pipeline-related flows. The console supports &lt;a href="https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0#"&gt;WhenExpressions&lt;/a&gt;, &lt;a href="https://github.com/tektoncd/pipeline/blob/main/docs/pipelines.md#configuring-a-pipeline"&gt;finally tasks&lt;/a&gt;, and more. &lt;code&gt;WhenExpressions&lt;/code&gt; are preceded by a diamond shape and &lt;code&gt;finally tasks&lt;/code&gt; are enclosed in a grouping with a white background, as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/whenfinally.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/whenfinally.png?itok=zFJTTKGg" width="600" height="408" alt="WhenExpressions and finally tasks can form a pipeline." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Pipeline with WhenExpressions and finally tasks. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;OpenShift GitOps&lt;/h2&gt; &lt;p&gt;With OpenShift GitOps 1.2 installed, you will see some updates to the &lt;strong&gt;Environments&lt;/strong&gt; page. You are now able to see the status of the environments to which each application has been deployed, as shown in Figure 6.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/environments.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/environments.png?itok=rpNIM1n8" width="600" height="299" alt="The Environments page shows information about each of an application's environments." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Environments page. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Managed Kafka&lt;/h2&gt; &lt;p&gt;We've provided an easy way for developers to &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka#provision_a_kafka_cluster_with_openshift_streams_for_apache_kafka_through_the_ui"&gt;create and scale event-driven applications with Apache Kafka&lt;/a&gt;. The &lt;a href="https://www.openshift.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt; Operator provides a streamlined developer experience for building, deploying, and scaling cloud-native applications in open, hybrid-cloud environments. This feature lets you connect with your application services directly in your own cluster by using the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/download"&gt;Red Hat OpenShift Application Services CLI&lt;/a&gt; and the OpenShift console UI.&lt;/p&gt; &lt;h2&gt;Customizing the developer perspective&lt;/h2&gt; &lt;p&gt;We are always looking for ways to provide a more streamlined user experience. In response to customer feedback, we now support two additional ways to customize the developer experience.&lt;/p&gt; &lt;h3&gt;Hiding features from the Add page&lt;/h3&gt; &lt;p&gt;Cluster admins can now hide features from the &lt;strong&gt;Add&lt;/strong&gt; page. To achieve this, add the &lt;code&gt;addPage&lt;/code&gt; customization in the Console spec. A code snippet is available in the YAML editor to perform this customization. That code snippet shows how to hide the &lt;strong&gt;Import from DevFile&lt;/strong&gt; entry, but you can adapt the code to your needs.&lt;/p&gt; &lt;h3&gt;Customizing roles in Project Access&lt;/h3&gt; &lt;p&gt;Cluster administrators can now customize which roles are being shown in &lt;strong&gt;Project Access&lt;/strong&gt; in the developer console. Default roles are Admin, Edit, and View. But the cluster administrator could configure a list of &lt;code&gt;ClusterRoles&lt;/code&gt; to override the default roles. To do so, add a &lt;code&gt;projectAccess&lt;/code&gt; customization in the console spec. A code snippet is available in the YAML editor to perform this customization.&lt;/p&gt; &lt;h2&gt;We want your feedback!&lt;/h2&gt; &lt;p&gt;Community feedback helps us continually improve the OpenShift developer experience, and we want to hear from you. You can attend our office hours on &lt;a href="http://openshift.tv/"&gt;Red Hat OpenShift Streaming&lt;/a&gt;, join the &lt;a href="https://groups.google.com/forum/#!forum/openshift-dev-users"&gt;OpenShift Developer Experience Google group&lt;/a&gt;, or &lt;a href="https://twitter.com/serenamarie125"&gt;tweet me your feedback directly&lt;/a&gt;. We hope you will share your tips for using the OpenShift web console, get help with what doesn’t work, and shape the future of the OpenShift developer experience. Ready to get started? &lt;a href="http://www.openshift.com/try"&gt;Try OpenShift today&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/02/whats-new-red-hat-openshift-48-console" title="What's new in the Red Hat OpenShift 4.8 console"&gt;What's new in the Red Hat OpenShift 4.8 console&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tM_WYj5RYNs" height="1" width="1" alt=""/&gt;</summary><dc:creator>Serena Chechile Nichols</dc:creator><dc:date>2021-08-02T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/02/whats-new-red-hat-openshift-48-console</feedburner:origLink></entry><entry><title type="html">Kogito 1.9.0 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/c6X4IGw2dL0/kogito-1-9-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2021/08/kogito-1-9-0-released.html</id><updated>2021-08-02T01:19:56Z</updated><content type="html">We are glad to announce that the Kogito 1.9.0 release is now available! This goes hand in hand with, . From a feature point of view, we included a series of new features and bug fixes, including: * New Task Assigning Service that enables you to automatically assign user tasks * Upgraded SpringBoot version to 2.3.10 * Adapter layer with Drools v7 API KNOWN ISSUES * Our team have identified an issue ) that currently prevents the usage of the different persistent addons on Quarkus native images. For more details head to the complete. All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.11.0 artifacts are available at the. A detailed changelog for 1.9.0 can be found in. New to Kogito? Check out our website. Click the "Get Started" button. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/c6X4IGw2dL0" height="1" width="1" alt=""/&gt;</content><dc:creator>Cristiano Nicolai</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/kogito-1-9-0-released.html</feedburner:origLink></entry><entry><title type="html">Bend-points and the DMN Editor</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Oixfk8_svDY/bend-points-and-the-dmn-editor.html" /><author><name>Daniel José dos Santos</name></author><id>https://blog.kie.org/2021/07/bend-points-and-the-dmn-editor.html</id><updated>2021-07-30T13:44:26Z</updated><content type="html">We’ve been working in DMN Editor to improve its user experience. We know that our users sometimes have models with many edges that overlap nodes, other edges and are hard to arrange. Even with features like the that help users on organizing the content, there are still some cases where users cannot or don’t want to split the diagram. But how can they deal with this? Users may connect the "NumC" and the "Basic" nodes, but is this the best way to organize the nodes? A simple solution is already available in the BPMN editor, and now implemented in the DMN editor relies on the use of bend-points for this kind of scenario. This feature enables users to create "flexible edges". It may be easier to reshape the edge by going around the other edges with this feature instead of rearranging the nodes, so that the edges do not overlap. PREVENTING THE PAIN The BPMN and the DMN editors rely on the same core diagram system, each of them with specific features for each use case. The beautiful thing is that when the BPMN introduced bend-points, we had in mind that it would be supported for DMN at some point too, so it was built in a way that it was decoupled from the BPMN editor, envisioning re-usability in other places than BPMN. So, on DMN, we have an Edge with a list of waypoints but only filled with two points: the source and the target node. Newcomers may think that developers from the past were doing bad coding using a list for keeping only two points, but it was made this way, keeping in mind the bend-points. So, when we finally enabled the bend-points, the DMN editor structure was ready to handle Edges of lists of waypoints. I like to point to this case and show how we, as developers, may keep our code ready for expansions, especially if it is a new feature that we already have on our radar like this one. It saves time for people from the future who maybe are ourselves. The support for bend-points will be available in the next Kogito release. Stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Oixfk8_svDY" height="1" width="1" alt=""/&gt;</content><dc:creator>Daniel José dos Santos</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/bend-points-and-the-dmn-editor.html</feedburner:origLink></entry><entry><title>Avoiding dual writes in event-driven applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aN8toxH78qM/avoiding-dual-writes-event-driven-applications" /><author><name>Bernard Tison</name></author><id>41c4e1a8-fdd0-49fb-9b56-053be5d91b86</id><updated>2021-07-30T07:00:00Z</updated><published>2021-07-30T07:00:00Z</published><summary type="html">&lt;p&gt;Dual writes frequently cause issues in distributed, event-driven applications. A &lt;em&gt;dual write&lt;/em&gt; occurs when an application has to change data in two different systems, such as when an application needs to persist data in the database and send a Kafka message to notify other systems. If one of these two operations fails, you might end up with inconsistent data. Dual writes can be hard to detect and fix.&lt;/p&gt; &lt;p&gt;In this article, you will learn how to use the outbox pattern with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; to avoid the dual write problem in event-driven applications. I will show you how to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Provision a Kafka cluster on OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Deploy and configure Debezium to use OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Run an application that uses Debezium and OpenShift Streams for Apache Kafka to implement the outbox pattern.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;OpenShift Streams for Apache Kafka&lt;/a&gt; is an Apache Kafka service that is fully hosted and managed by Red Hat. The service is useful for developers who want to incorporate streaming data and scalable messaging in their applications without the burden of setting up and maintaining a Kafka cluster infrastructure.&lt;/p&gt; &lt;p&gt;&lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; is an open source, distributed platform for change data capture. Built on top of Apache Kafka, Debezium allows applications to react to inserts, updates, and deletes in your databases.&lt;/p&gt; &lt;h2&gt;Demo: Dual writes in an event-driven system&lt;/h2&gt; &lt;p&gt;The demo application we'll use in this article is part of a distributed, event-driven order management system. The application suffers from the dual write issue: When a new order comes in through a REST interface, the order is persisted in the database—in this case PostgreSQL—and an &lt;code&gt;OrderCreated&lt;/code&gt; event is sent to a Kafka topic. From there, it can be consumed by other parts of the system.&lt;/p&gt; &lt;p&gt;You will find the code for the order service application in this &lt;a href="https://github.com/rhosak-debezium-outbox/order-service"&gt;Github repository&lt;/a&gt;. The application was developed using &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;. The structure of the &lt;code&gt;OrderCreated&lt;/code&gt; events follows the &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt; specification, which defines a common way to describe event data.&lt;/p&gt; &lt;h3&gt;Solving dual writes with the outbox pattern&lt;/h3&gt; &lt;p&gt;Figure 1 shows the architecture of the outbox pattern implemented with Debezium and OpenShift Streams for Apache Kafka. For this example, you will also use Docker to spin up the different application components on your local system.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/arch.png?itok=LtcRgFRW" width="600" height="338" alt="In the outbox pattern with Apache Kafka, Debezium monitors inserts and informs Kafka, which in turn tells the event consumers about the change." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Architecture of the outbox pattern with Debezium and OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: For a thorough discussion of Debezium and the outbox pattern see &lt;a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/"&gt;Reliable Microservices Data Exchange With the Outbox Pattern&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Prerequisites for the demonstration&lt;/h3&gt; &lt;p&gt;This article assumes that you already have an OpenShift Streams for Apache Kafka instance in your development environment. Visit the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; page to create a Kafka instance. See the article &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; for the basics of creating Kafka instances, topics, and service accounts.&lt;/p&gt; &lt;p&gt;I also assume that you have Docker installed on your local system.&lt;/p&gt; &lt;h2&gt;Provision a Kafka cluster with OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;After you set up an OpenShift Streams for Apache Kafka instance, you will create environment variables for the Kafka bootstrap server endpoint and the service account credentials. Use the following environment variables when configuring the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export KAFKA_BOOTSTRAP_SERVER=&lt;value of the Bootstrap server endpoint&gt; $ export CLIENT_ID=&lt;value of the service account Client ID&gt; $ export CLIENT_SECRET=&lt;value of the service account Client Secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a topic named &lt;code&gt;order-event&lt;/code&gt; on your Kafka instance for the user service application, as shown in Figure 2. The number of partitions is not critical for this example. I generally use 15 partitions for Kafka topics as a default. You can leave the message retention time at seven days.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topic.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/topic.png?itok=5CDjuuoC" width="600" height="392" alt="The Kafka instance for the outbox pattern shows information about the order-event." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The order-event Kafka topic on OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Persisting the order service&lt;/h3&gt; &lt;p&gt;The order service application exposes a REST endpoint for new orders. When a new order is received, the order is persisted using JPA in the &lt;code&gt;orders&lt;/code&gt; table of the PostgreSQL database. In the same transaction, the outbox event for the &lt;code&gt;OrderCreated&lt;/code&gt; message is written to the &lt;code&gt;orders_outbox&lt;/code&gt; table. See &lt;a href="https://github.com/rhosak-debezium-outbox/order-service"&gt;this Github repo&lt;/a&gt; for the application source code. The &lt;code&gt;OrderService&lt;/code&gt; class contains the code for persisting the order entity and the &lt;code&gt;OrderCreated&lt;/code&gt; message:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@ApplicationScoped public class OrderService { @Inject EntityManager entityManager; @ConfigProperty(name = "order-event.aggregate.type", defaultValue = "order-event") String aggregateType; @Transactional public Long create(Order order) { order.setStatus(OrderStatus.CREATED); entityManager.persist(order); OutboxEvent outboxEvent = buildOutBoxEvent(order); entityManager.persist(outboxEvent); entityManager.remove(outboxEvent); return order.getId(); } OutboxEvent buildOutBoxEvent(Order order) { OutboxEvent outboxEvent = new OutboxEvent(); outboxEvent.setAggregateType(aggregateType); outboxEvent.setAggregateId(Long.toString(order.getId())); outboxEvent.setContentType("application/cloudevents+json; charset=UTF-8"); outboxEvent.setPayload(toCloudEvent(order)); return outboxEvent; } String toCloudEvent(Order order) { CloudEvent event = CloudEventBuilder.v1().withType("OrderCreatedEvent") .withTime(OffsetDateTime.now()) .withSource(URI.create("ecommerce/order-service")) .withDataContentType("application/json") .withId(UUID.randomUUID().toString()) .withData(order.toJson().encode().getBytes()) .build(); EventFormat format = EventFormatProvider.getInstance() .resolveFormat(JsonFormat.CONTENT_TYPE); return new String(format.serialize(event)); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The structure of the &lt;code&gt;OrderCreated&lt;/code&gt; message follows the CloudEvents specification. The code uses the Java SDK for CloudEvents API to build the CloudEvent and serialize it to JSON format.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ContentType&lt;/code&gt; field of the &lt;code&gt;OutboxEvent&lt;/code&gt; entity is set to &lt;code&gt;application/cloudevents+json&lt;/code&gt;. When processed by a Debezium single message transformation (SMT), this value is set as the &lt;code&gt;content-type&lt;/code&gt; header on the Kafka message, as mandated by the CloudEvents specification.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I'll discuss the structure of the Outbox Event table shortly. The &lt;code&gt;OutboxEvent&lt;/code&gt; entity is persisted to the database and then removed right away. Debezium, being log-based, does not examine the contents of the database table; it just tails the append-only transaction log. The code will generate an &lt;code&gt;INSERT&lt;/code&gt; and a &lt;code&gt;DELETE&lt;/code&gt; entry in the log when the transaction commits. Debezium processes both events, and produces a Kafka message for any &lt;code&gt;INSERT&lt;/code&gt;. However, &lt;code&gt;DELETE&lt;/code&gt; events are ignored.&lt;/p&gt; &lt;p&gt;The net result is that Debezium is able to capture the event added to the outbox table, but the table itself remains empty. No additional disk space is needed for the table and no separate housekeeping process is required to stop it from growing indefinitely.&lt;/p&gt; &lt;h3&gt;Running the PostgreSQL database&lt;/h3&gt; &lt;p&gt;Run the PostgreSQL database as a Docker container. The database and the &lt;code&gt;orders&lt;/code&gt; and &lt;code&gt;orders_outbox&lt;/code&gt; tables are created when the container starts up:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name postgresql \ -e POSTGRESQL_USER=orders -e POSTGRESQL_PASSWORD=orders \ -e POSTGRESQL_ADMIN_PASSWORD=admin -e POSTGRESQL_DATABASE=orders \ quay.io/btison_rhosak/postgresql-order-service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, spin up the container for the order service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name order-service -p 8080:8080 \ --link postgresql -e DATABASE_USER=orders \ -e DATABASE_PASSWORD=orders -e DATABASE_NAME=orders \ -e DATABASE_HOST=postgresql -e ORDER_EVENT_AGGREGATE_TYPE=order-event \ quay.io/btison_rhosak/order-service-outbox&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configure and run Debezium&lt;/h2&gt; &lt;p&gt;Debezium is implemented as a &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Kafka Connect&lt;/a&gt; connector, so the first thing to do is to spin up a Kafka Connect container, pointing to the managed Kafka instance.&lt;/p&gt; &lt;p&gt;The Kafka Connect image we use here is derived from the Kafka Connect image provided by the &lt;a href="https://strimzi.io"&gt;Strimzi&lt;/a&gt; project. The Debezium libraries and the Debezium PostgreSQL connector are already installed on this image.&lt;/p&gt; &lt;p&gt;Kafka Connect is configured using a properties file, which specifies among other things how Kafka Connect should connect to the Kafka broker.&lt;/p&gt; &lt;p&gt;To connect to the managed Kafka instance, this application employs &lt;a href="https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_plain.html"&gt;SASL/PLAIN&lt;/a&gt; authentication over TLS, using the client ID and secret from the service account you created earlier as credentials.&lt;/p&gt; &lt;h3&gt;Creating a configuration file&lt;/h3&gt; &lt;p&gt;Create a configuration file on your local file system. The file refers to environment variables for the Kafka bootstrap address and the service account credentials:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ cat &lt;&lt; EOF &gt; /tmp/kafka-connect.properties # Bootstrap servers bootstrap.servers=$KAFKA_BOOTSTRAP_SERVER # REST Listeners rest.port=8083 # Plugins plugin.path=/opt/kafka/plugins # Provided configuration offset.storage.topic=kafka-connect-offsets value.converter=org.apache.kafka.connect.json.JsonConverter config.storage.topic=kafka-connect-configs key.converter=org.apache.kafka.connect.json.JsonConverter group.id=kafka-connect status.storage.topic=kafka-connect-status config.storage.replication.factor=3 key.converter.schemas.enable=false offset.storage.replication.factor=3 status.storage.replication.factor=3 value.converter.schemas.enable=false security.protocol=SASL_SSL producer.security.protocol=SASL_SSL consumer.security.protocol=SASL_SSL admin.security.protocol=SASL_SSL sasl.mechanism=PLAIN producer.sasl.mechanism=PLAIN consumer.sasl.mechanism=PLAIN admin.sasl.mechanism=PLAIN sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; admin.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; EOF&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Launching the Kafka Connect container&lt;/h3&gt; &lt;p&gt;Next, you will launch the Kafka Connect container. The properties file you just created is mounted into the container. The container is also linked to the PostgreSQL container, so that the Debezium connector can connect to PostgreSQL to access the transaction logs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name kafka-connect --link postgresql -p 8083:8083 \ --mount type=bind,source=/tmp/kafka-connect.properties,destination=/config/kafka-connect.properties \ quay.io/btison_rhosak/kafka-connect-dbz-pgsql:1.7.0-1.5.0.Final&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now check the Kafka Connect container logs. Kafka Connect logs are quite verbose, so if you don’t see any stack traces, you can assume Kafka Connect is running fine and successfully connected to the Kafka cluster.&lt;/p&gt; &lt;h3&gt;Configuring the Debezium connector&lt;/h3&gt; &lt;p&gt;Kafka Connect exposes a REST endpoint through which you can deploy and manage Kafka Connect connectors, such as the Debezium connector. Create a file on your local file system for the Debezium connector configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ cat &lt;&lt; EOF &gt; /tmp/debezium-connector.json { "name": "debezium-postgres-orders", "config": { "connector.class": "io.debezium.connector.postgresql.PostgresConnector", "plugin.name": "pgoutput", "database.hostname": "postgresql", "database.port": "5432", "database.user": "postgres", "database.password": "admin", "database.dbname": "orders", "database.server.name": "orders1", "schema.whitelist": "public", "table.whitelist": "public.orders_outbox", "tombstones.on.delete" : "false", "transforms": "router", "transforms.router.type": "io.debezium.transforms.outbox.EventRouter", "transforms.router.table.fields.additional.placement": "content_type:header:content-type", "transforms.router.route.topic.replacement": "\${routedByValue}" } } EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's look more closely at some of the fields in this configuration:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;plugin-name&lt;/strong&gt;: The Debezium connector uses a PostgreSQL output plug-in to extract changes committed to the transaction log. In this case, we use the &lt;code&gt;pgoutput&lt;/code&gt; plug-in, which is included in PostgreSQL since version 10. See the &lt;a href="https://debezium.io/documentation/reference/1.5/connectors/postgresql.html"&gt;Debezium documentation&lt;/a&gt; for information about output plug-ins.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;database.*&lt;/strong&gt;: These settings allow Debezium to connect to the PostgreSQL database. This example specifies the &lt;code&gt;postgres&lt;/code&gt; system user, which has superuser privileges. In a production system, you should probably create a dedicated Debezium user with the necessary privileges.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;table.whitelist&lt;/strong&gt;: This specifies the list of tables that are monitored for changes by the Debezium Connector.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;tombstones.on.delete&lt;/strong&gt;: This indicates whether a deletion marker ("tombstones") should be emitted by the connector when a record is deleted from the outbox table. By setting &lt;code&gt;tombstones.on.delete&lt;/code&gt; to false, you tell Debezium to effectively ignore deletes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;transforms.*&lt;/strong&gt;: These settings describe how Debezium should process database change events. Debezium applies a single message transform (SMT) to every captured change event. For the outbox pattern, Debezium uses the built-in &lt;code&gt;EventRouter&lt;/code&gt; SMT, which extracts the new state of the change event, transforms it into a Kafka message, and sends it to the appropriate topic.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;More about the EventRouter&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;EventRouter&lt;/code&gt; by default makes certain assumptions about the structure of the outbox table:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; Column | Type | Modifiers --------------+------------------------+----------- id | uuid | not null aggregatetype | character varying(255) | not null aggregateid | character varying(255) | not null payload | text | not null content_type | character varying(255) | not null &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;EventRouter&lt;/code&gt; calculates the value of the destination topic from the &lt;code&gt;aggregatetype&lt;/code&gt; column and the value of the &lt;code&gt;route.topic.replacement&lt;/code&gt; configuration (where &lt;code&gt;${routedBy}&lt;/code&gt; represents the value in the &lt;code&gt;aggregatetype&lt;/code&gt; column). The key of the Kafka message is the value of the &lt;code&gt;aggregateid&lt;/code&gt; column, and the payload is whatever is in the &lt;code&gt;payload&lt;/code&gt; column. The &lt;code&gt;table.fields.additional.placement&lt;/code&gt; parameter defines how additional columns should be handled. In our case, we specify that the value of the &lt;code&gt;content_type&lt;/code&gt; column should be added to the Kafka message as a header with key &lt;code&gt;content-type&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Deploying the Debezium connector&lt;/h3&gt; &lt;p&gt;Deploy the Debezium connector by calling the Kafka Connect REST endpoint:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X POST -H "Accept: application/json" -H "Content-type: application/json" \ -d @/tmp/debezium-connector.json 'http://localhost:8083/connectors'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the logs of the &lt;code&gt;kafka-connect&lt;/code&gt; container to verify that the Debezium connector was installed successfully. If you were successful, you’ll see something like the following toward the end of the logs:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;2021-06-09 21:09:46,944 INFO user 'postgres' connected to database 'orders' on PostgreSQL 12.5 on x86_64-redhat-linux-gnu, compiled by gcc (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5), 64-bit with roles: role 'pg_read_all_settings' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_stat_scan_tables' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_write_server_files' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_monitor' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_read_server_files' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'orders' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: true] role 'pg_execute_server_program' [superuser: false, replication: false, inherit: true, create role: false, create db: false,can log in: false] role 'pg_read_all_stats' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can login: false] role 'pg_signal_backend' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can login: false] role 'postgres' [superuser: true, replication: true, inherit: true, create role: true, create db: true, can log in: true] (io.debezium.connector.postgresql.PostgresConnectorTask) [task-thread-debezium-postgres-orders-0] &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Consume the Kafka messages&lt;/h2&gt; &lt;p&gt;Next, you want to view the Kafka messages produced by Debezium and sent to the &lt;code&gt;order-event&lt;/code&gt; topic. You can use a tool such as &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt; to process the messages. The following &lt;code&gt;docker&lt;/code&gt; command launches a container hosting the &lt;code&gt;kafkacat&lt;/code&gt; utility and consumes all the messages in the &lt;code&gt;order-event&lt;/code&gt; topic from the beginning:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -it --rm edenhill/kafkacat:1.6.0 kafkacat \ -b $KAFKA_BOOTSTRAP_SERVER -t order-event \ -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN \ -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" \ -f 'Partition: %p\n Key: %k\n Headers: %h\n Payload: %s\n' -C&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Test the application&lt;/h2&gt; &lt;p&gt;All components are in place to test the order service application. To use the REST interface to create an order, issue the following cURL command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -v -X POST -H "Content-type: application/json" \ -d '{"customerId": "customer123", "productCode": "XXX-YYY", "quantity": 3, "price": 159.99}' \ http://localhost:8080/order&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Verify with &lt;code&gt;kafkacat&lt;/code&gt; that a Kafka message has been produced to the &lt;code&gt;order-event&lt;/code&gt; topic. When you issue the &lt;code&gt;kafkacat&lt;/code&gt; command mentioned earlier, the output should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Partition: 3 Key: "992" Headers: id=743e3736-f9e3-4c2f-bce7-eaa35afe8876,content-type=application/cloudevents+json; charset=UTF-8 Payload: "{\"specversion\":\"1.0\",\"id\":\"843d8770-f23d-41e2-a697-a64367f1d387\",\"source\":\"ecommerce/order-service\",\"type\":\"OrderCreatedEvent\",\"datacontenttype\":\"application/json\",\"time\":\"2021-06-10T07:40:52.282602Z\",\"data\":{\"id\":992,\"customerId\":\"customer123\",\"productCode\":\"XXX-YYY\",\"quantity\":3,\"price\":159.99,\"status\":\"CREATED\"}}" % Reached end of topic order-event [3] at offset 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the ID of the outbox event is added as a header to the message. This information can be exploited by consumers for duplicate detection. The &lt;code&gt;content-type&lt;/code&gt; header is added by the Debezium &lt;code&gt;EventRouter&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Great job! If you’ve followed along, you have successfully:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Provisioned a &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;managed Kafka&lt;/a&gt; instance on &lt;a href="https://cloud.redhat.com"&gt;cloud.redhat.com&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Used &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Kafka Connect&lt;/a&gt; and &lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; to connect to the managed Kafka instance.&lt;/li&gt; &lt;li&gt;Implemented and tested the outbox pattern with Debezium.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Setting up and maintaining a Kafka cluster can be tedious and complex. OpenShift Streams for Apache Kafka takes away that burden, allowing you to focus on implementing services and business logic.&lt;/p&gt; &lt;p&gt;Applications can connect to the managed Kafka instance from everywhere, so it doesn’t really matter where these applications run, whether it's on a private or public cloud, or even in Docker containers on your local workstation.&lt;/p&gt; &lt;p&gt;Stay tuned for more articles on interesting use cases and demos with OpenShift Streams for Apache Kafka.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications" title="Avoiding dual writes in event-driven applications"&gt;Avoiding dual writes in event-driven applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aN8toxH78qM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Bernard Tison</dc:creator><dc:date>2021-07-30T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications</feedburner:origLink></entry><entry><title type="html">Why some prometheus alerts in k8s can confuse</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FDn2otUDpuU/" /><author><name /></author><id>https://blog.ramon-gordillo.dev/2021/07/why-some-prometheus-alerts-in-k8s-can-confuse/</id><updated>2021-07-30T00:00:00Z</updated><content type="html">Recently I was installing a kubernetes cluster as I usually do for my tests. However, as those machines were bare metal servers that some colleagues have recycled, we decided to keep it running and try to maintain it by ourselves. First thing I did was a simple bot to send the alerts to a telegram channel. That was something I did not do in the past, because I do not care about monitoring as my clusters were ephemeral. After two days, an alert started firing. This blog is my analysis of what this alert means and why I consider it not accurate, so I ended silencing it (but I would love to have an accurate alert for that situation). THE ALERT The alert in question is KubeMemoryOvercommit. It is defined in this , and its description says: Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure. The rule expression for this alert is sum(namespace_memory:kube_pod_container_resource_requests:sum{}) / sum(kube_node_status_allocatable{resource="memory"}) &gt; ((count(kube_node_status_allocatable{resource="memory"}) &gt; 1) - 1) / count(kube_node_status_allocatable{resource="memory"}) Let’s try to understand what it means. THE CALCULATIONS Looking at the different members of the equation, we can see what it means: sum(namespace_memory:kube_pod_container_resource_requests:sum{}) = sum of memory requests in bytes for all pods in every namespace sum(kube_node_status_allocatable{resource="memory"}) = sum of memory that can be allocated to pods in every nodeernel Dividing both, we obtain the ratio of memory allocated related with the total amount that can be. Let’s go for the second part. count(kube_node_status_allocatable{resource="memory"}) = number of nodes that can be used to deploy pods ((count(kube_node_status_allocatable{resource="memory"}) &gt; 1) - 1) = number of nodes that can be used to deploy pods minus one, which should be at least one = remaining nodes which can allocate pods in case one is down Dividing the latter by the former, we got a ratio of (nodes - 1)/nodes Having the details in mind, a summary of this equation is It seems very easy to understand, but it is accurate? Let’s see what it is not in the formula. WHAT IS NOT CONSIDERED? I will try to summarize some of my thoughts in bullets. For the first part, to know if pods of a node can be reallocatable or not, there are lots of different concepts to bear in mind: * Daemonsets deploys pods per node, and this pod is meant for that node. It cannot be moved. * NodeSelector can be used to restrict which nodes a pod can land into. * Affinity/anti-affinity rules can also restrict where pods can be deployed * Taints and tolerations can avoid pods to be scheduled in those nodes. * Special hardware requirements, for example GPUs, are also restricted to some nodes of the cluster. Rules for deploying use to be based on For the second part of the equation: * Nodes can be different in size, so the second part of the equation is not accurate either. For example, if we have 1 nodes of 512 Gb RAM and 1 of 64 Gb RAM, if you have one half full and one empty (which you can see it will not fire the alert), if the full node is the bigger one and it goes down, you cannot reallocate the pods in the smaller, but it will be possible on the opposite direction. CONCLUSION I find it very useful to know if the platform has any risk of overcommitting the workloads if a node shuts down (either abruptly or planned). I find it particularly relevant in upgrades, where nodes usually have to be rebooted during the upgrade in a rolling process. However, as I have shown previously, there are so many factors to consider than a simple rule based on a couple of metrics gives a false security feeling. The only “accurate” solution that comes to my mind is if we can tell the scheduler to simulate the drain of a node and throw any issues that could be foreseen, like an extension of what is with kubectl drain --dry-run&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FDn2otUDpuU" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://blog.ramon-gordillo.dev/2021/07/why-some-prometheus-alerts-in-k8s-can-confuse/</feedburner:origLink></entry><entry><title>Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3oYKYRb3wLU/troubleshooting-application-performance-red-hat-openshift-metrics-part-4" /><author><name>Pavel Macik</name></author><id>66873ac1-84f5-4f2b-83ad-fb28e2a5b5a7</id><updated>2021-07-29T07:00:00Z</updated><published>2021-07-29T07:00:00Z</published><summary type="html">&lt;p&gt;This series shows how to use &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; metrics in a real-life performance testing scenario. I used these metrics to run performance tests on the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator&lt;/a&gt;. We used the results to performance-tune the Service Binding Operator for acceptance into the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Part 3&lt;/a&gt;, I showed you how we set up &lt;a href="https://docs.openshift.com/container-platform/4.7/monitoring/understanding-the-monitoring-stack.html?extIdCarryOver=true&amp;intcmp=7013a000002w3nnAAA&amp;sc_cid=7013a000002w0ZmAAI"&gt;OpenShift's monitoring stack&lt;/a&gt; to collect runtime metrics for our testing scenarios. I also shared a collector script that ensures the results are preserved on a node that won't crash. Now, we can look at the performance metrics we'll use and how to gather the data we need.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the whole series&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1"&gt;Performance requirements&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2021/07/15/troubleshooting-application-performance-red-hat-openshift-metrics-part-2-test"&gt;The test environment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Collecting runtime metrics&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Part 4: Gathering performance metrics&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Part 5: Test rounds and results (August 5)&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;CPU and memory usage&lt;/h2&gt; &lt;p&gt;As mentioned in &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Part 3&lt;/a&gt;, some metrics have to be collected for the duration of the test. The most important metrics required by the Developer Sandbox team were the CPU and memory usage of both OpenShift nodes and the tested operators. We also wanted to see the number of resources left in the cluster after the test was done.&lt;/p&gt; &lt;p&gt;Unfortunately, we ran into problems during our first attempts to run the stress tests. The OpenShift cluster actually crashed when one of its worker nodes came down. Naturally, it was important to know what caused the failure. I needed a more granular view of the CPU and memory usage, so I had to collect data from the workloads deployed on those nodes.&lt;/p&gt; &lt;p&gt;From watching and inspection using the cluster's own &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; instance, I identified a couple of resources that were loaded and stressed by our scenario. Then, I included them to be watched by the &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3#collecting_runtime_metrics_with_the_openshift_monitoring_tool"&gt;collector script&lt;/a&gt; I shared in the previous article, together with the cluster's nodes.&lt;/p&gt; &lt;p&gt;I identified workloads from a handful of namespaces as stressed, and included them to be watched for CPU and memory usage. Table 1 shows these namespaces and workloads.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 1: Workloads to watch for CPU and memory usage.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;th scope="col"&gt;Namespace&lt;/th&gt; &lt;th scope="col"&gt;Workloads&lt;/th&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-apiserver&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;apiserver-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-kube-apiserver&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;kube-apiserver-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-monitoring&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;prometheus-k8s-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-operators&lt;/code&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;service-binding-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;li&gt;&lt;code&gt;rhoas-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-operator-lifecycle-manager&lt;/code&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;catalog-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;li&gt;&lt;code&gt;olm-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Resources created in the cluster&lt;/h2&gt; &lt;p&gt;Another one of the metrics requested by the Developer Sandbox team was the number of resources created in the cluster during the test. There were two ways to get that information:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;If the Prometheus instance was available (meaning the node on which it is deployed had not crashed), I could use a simple Prometheus query: &lt;pre&gt; &lt;code&gt;sort_desc(cluster:usage:resources:sum)&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;If the Prometheus instance was not available, I had to employ a brute-force approach, using the &lt;code&gt;oc&lt;/code&gt; tool to count the number of each resource.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Service Binding Operator performance and developer experience&lt;/h2&gt; &lt;p&gt;The final set of metrics was related to the performance of the Service Binding Operator itself, from a developer's perspective. Specifically, we wanted to know how long it took to perform the binding after the &lt;code&gt;ServiceBinding&lt;/code&gt; resource was created.&lt;/p&gt; &lt;p&gt;The typical situation for a developer using the Service Binding Operator is to have a backing service and an application running that the user wants to bind together. So, the developer sends a &lt;code&gt;ServiceBinding&lt;/code&gt; request and expects the binding to be done by Service Binding Operator. The scenario can be split into the following sequence of steps. Each step is shown along with the way to retrieve the respective timestamp:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;The &lt;code&gt;ServiceBinding&lt;/code&gt; request is sent, processed by OpenShift, and created internally as a resource (&lt;code&gt;.metadata.creationTimestamp&lt;/code&gt; of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource).&lt;/li&gt; &lt;li&gt;The Service Binding Operator picks up the resource while watching for it and processes it. (This is the first "Reconciling ServiceBinding" message for the particular &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the Service Binding Operator logs.)&lt;/li&gt; &lt;li&gt;Based on the content of the resource, the Service Binding Operator performs the binding. It collects bindable information from the backing service and injects it into the application.&lt;/li&gt; &lt;li&gt;The application is re-deployed with the bound information injected into the &lt;code&gt;Deployment&lt;/code&gt; resource (&lt;code&gt;.status.conditions[] | select(.type=="Available") | select(.status=="True").lastTransitionTime&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ServiceBinding&lt;/code&gt; resource is marked as done. (A "Done" message is sent for the particular &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the Service Binding Operator logs.)&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;I defined the following metrics to evaluate the developer experience in this scenario:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Time to Ready&lt;/strong&gt;: The time between the creation of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource (1) and completion of the binding (5). That can be further split into the following: &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Time to Reconcile&lt;/strong&gt;: The time between the creation of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource (1) and when it is picked up by Service Binding Operator (2).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reconcile to Ready&lt;/strong&gt;: The time between when the Service Binding Operator picks up the &lt;code&gt;ServiceBinding&lt;/code&gt; (2) and completes the binding (5).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;At the time of the performance evaluation, these metrics were not collected by OpenShift's monitoring stack or by the Service Binding Operator. So, I had to dig up the information from the data that I could get. As shown earlier, I derived the information I needed from metadata that OpenShift collects about the active user's backing service and the application (especially timestamps), along with the Service Binding Operator logs. I wrote the following script to collect the necessary data from OpenShift and compute the information after the test is complete:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;if [ -z "$QUAY_NAMESPACE" ]; then echo "QUAY_NAMESPACE environemnt variable needs to be set to a non-empty value" exit 1 fi DT=$(date "+%F_%T") RESULTS=results-$DT mkdir -p $RESULTS USER_NS_PREFIX=${1:-zippy} # Resource counts resource_counts(){ echo -n "$1;" # All resource counts from user namespaces echo -n "$(oc get $1 --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace | grep $USER_NS_PREFIX | wc -l)" echo -n ";" # All resource counts from all namespaces echo "$(oc get $1 --all-namespaces -o name | wc -l)" } # Dig various timestamps out timestamps(){ SBR_JSON=$1 DEPLOYMENTS_JSON=$2 SBO_LOG=$3 RESULTS=$4 jq -rc '((.metadata.namespace) + ";" + (.metadata.name) + ";" + (.metadata.creationTimestamp) + ";" + (.status.conditions[] | select(.type=="Ready").lastTransitionTime))' $SBR_JSON &gt; $RESULTS/tmp.csv echo "ServiceBinding;Created;ReconciledTimestamp;Ready;AllDoneTimestamp" &gt; $RESULTS/sbr-timestamps.csv for i in $(cat $RESULTS/tmp.csv); do ns=$(echo -n $i | cut -d ";" -f1) name=$(echo -n $i | cut -d ";" -f2) echo -n $ns/$name; echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f3) "+%F %T"); echo -n ";"; log=$(cat $SBO_LOG | grep $ns) date -d @$(echo $log | jq -rc 'select(.msg | contains("Reconciling")).ts' | head -n1) "+%F %T.%N" | tr -d "\n" echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f4) "+%F %T"); echo -n ";"; done_ts=$(echo $log | jq -rc 'select(.msg | contains("Done")) | select(.retry==false).ts') if [ -n "$done_ts" ]; then date -d "@$done_ts" "+%F %T.%N" else echo "" fi done &gt;&gt; $RESULTS/sbr-timestamps.csv rm -f $RESULTS/tmp.csv jq -rc '((.metadata.namespace) + ";" + (.metadata.name) + ";" + (.metadata.creationTimestamp) + ";" + (.status.conditions[] | select(.type=="Available") | select(.status=="True").lastTransitionTime)) + ";" + (.metadata.managedFields[] | select(.manager=="manager").time)' $DEPLOYMENTS_JSON &gt; $RESULTS/tmp.csv echo "Namespace;Deployment;Deployment_Created;Deployment_Available;Deployment_Updated_by_SBO;SB_Name;SB_created;SB_ReconciledTimestamp;SB_Ready;SB_AllDoneTimestamp" &gt; $RESULTS/binding-timestamps.csv for i in $(cat $RESULTS/tmp.csv); do NS=$(echo -n $i | cut -d ";" -f1); echo -n $NS; echo -n ";"; echo -n $(echo -n $i | cut -d ";" -f2); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f3) "+%F %T"); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f4) "+%F %T"); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f5) "+%F %T"); echo -n ";"; cat $RESULTS/sbr-timestamps.csv | grep $NS done &gt;&gt; $RESULTS/binding-timestamps.csv rm -f $RESULTS/tmp.csv } # Collect timestamps { # ServiceBinding resources in user namespaces oc get sbr --all-namespaces -o json | jq -r '.items[] | select(.metadata.namespace | contains("'$USER_NS_PREFIX'"))' &gt; $RESULTS/service-bindings.json # Deployment resources in user namespaces oc get deploy --all-namespaces -o json | jq -r '.items[] | select(.metadata.namespace | contains("'$USER_NS_PREFIX'"))' &gt; $RESULTS/deployments.json # ServiceBiding operator log oc logs $(oc get $(oc get pods -n openshift-operators -o name | grep service-binding-operator) -n openshift-operators -o jsonpath='{.metadata.name}') -n openshift-operators &gt; $RESULTS/service-binding-operator.log timestamps $RESULTS/service-bindings.json $RESULTS/deployments.json $RESULTS/service-binding-operator.log $RESULTS } &amp; # Collect resource counts { RESOURCE_COUNTS_OUT=$RESULTS/resource-count.csv echo "Resource;UserNamespaces;AllNamespaces" &gt; $RESOURCE_COUNTS_OUT for i in $(cat resources.list); do resource_counts $i &gt;&gt; $RESOURCE_COUNTS_OUT; done } &amp; wait &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;This article introduced the metrics we collected to performance-test the Service Binding Operator for acceptance into the Developer Sandbox for Red Hat OpenShift. I showed how we collected both metrics of interest to the Developer Sandbox team, and additional metrics for our team specifically. Look for Part 5, the final article in this series, where I will present the testing rounds and their results.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/29/troubleshooting-application-performance-red-hat-openshift-metrics-part-4" title="Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics"&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3oYKYRb3wLU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Pavel Macik</dc:creator><dc:date>2021-07-29T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/29/troubleshooting-application-performance-red-hat-openshift-metrics-part-4</feedburner:origLink></entry></feed>
